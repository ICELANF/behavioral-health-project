"""
è¡Œå¥å¹³å° â€” å¥‘çº¦æ³¨å†Œè¡¨ä¿¡æ¯æå–å·¥å…·åŒ…
=======================================
ç”¨é€”: ä»ä»£ç åº“å’Œè¿è¡Œä¸­çš„å¹³å°æå–6ç±»å…³é”®ä¿¡æ¯
è¿è¡Œ: python extract_platform_info.py --project-root D:\behavioral-health-project
è¾“å‡º: åœ¨é¡¹ç›®æ ¹ç›®å½•ç”Ÿæˆ _contract_extraction/ æ–‡ä»¶å¤¹ï¼ŒåŒ…å«6ä¸ªç»“æ„åŒ–JSONæ–‡ä»¶

å‰ç½®æ¡ä»¶:
  1. Python 3.10+
  2. pip install requests pyyaml  (å¦‚æœè¿˜æ²¡è£…)
  3. å¹³å°å¦‚æœæ­£åœ¨è¿è¡Œï¼Œå¯ä»¥é¢å¤–æå–è¿è¡Œæ—¶ä¿¡æ¯(å¯é€‰)
"""

import os
import re
import sys
import json
import glob
import ast
import argparse
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é…ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OUTPUT_DIR_NAME = "_contract_extraction"
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")


def ensure_output_dir(project_root):
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    out = Path(project_root) / OUTPUT_DIR_NAME
    out.mkdir(exist_ok=True)
    return out


def save_json(out_dir, filename, data):
    """ä¿å­˜JSONæ–‡ä»¶"""
    path = out_dir / filename
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=str)
    print(f"  âœ… å·²ä¿å­˜: {path}")
    return path


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æå–å™¨ 1: æ•°æ®æ¨¡å‹ (SQLAlchemy Models)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_data_models(project_root):
    """
    æ‰«ææ‰€æœ‰ .py æ–‡ä»¶ï¼Œæå– SQLAlchemy æ¨¡å‹å®šä¹‰
    è¯†åˆ«: class XxxModel(Base) æˆ– class Xxx(db.Model) ç­‰æ¨¡å¼
    """
    print("\nğŸ“Š [1/6] æå–æ•°æ®æ¨¡å‹...")
    
    models = []
    model_files = []
    
    # å¸¸è§çš„SQLAlchemyåŸºç±»æ¨¡å¼
    base_patterns = [
        r'class\s+(\w+)\s*\(\s*(?:Base|db\.Model|DeclarativeBase|SQLModel)\s*\)',
        r'class\s+(\w+)\s*\(\s*\w*Base\w*\s*\)',
        r'class\s+(\w+)\s*\(.*Mixin.*Base.*\)',
    ]
    
    # å­—æ®µæ¨¡å¼
    column_pattern = re.compile(
        r'(\w+)\s*[=:]\s*(?:Column|mapped_column|Field)\s*\(\s*(\w+)'
    )
    relationship_pattern = re.compile(
        r'(\w+)\s*[=:]\s*(?:relationship|Relationship)\s*\(\s*["\'](\w+)["\']'
    )
    fk_pattern = re.compile(
        r'ForeignKey\s*\(\s*["\'](\w+\.\w+)["\']'
    )
    
    for py_file in glob.glob(str(Path(project_root) / "**" / "*.py"), recursive=True):
        if any(skip in py_file for skip in ["__pycache__", "node_modules", ".venv", "venv", "migrations"]):
            continue
            
        try:
            with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        except:
            continue
        
        for pattern in base_patterns:
            for match in re.finditer(pattern, content):
                model_name = match.group(1)
                
                # æå–è¯¥ç±»çš„å­—æ®µ
                # æ‰¾åˆ°ç±»å®šä¹‰çš„èŒƒå›´(ç®€åŒ–: ä»classåˆ°ä¸‹ä¸€ä¸ªclassæˆ–æ–‡ä»¶æœ«å°¾)
                class_start = match.start()
                next_class = re.search(r'\nclass\s+', content[class_start+10:])
                class_end = class_start + 10 + next_class.start() if next_class else len(content)
                class_body = content[class_start:class_end]
                
                # è¡¨å
                tablename_match = re.search(r'__tablename__\s*=\s*["\'](\w+)["\']', class_body)
                tablename = tablename_match.group(1) if tablename_match else model_name.lower()
                
                # å­—æ®µ
                columns = []
                for col_match in column_pattern.finditer(class_body):
                    columns.append({
                        "name": col_match.group(1),
                        "type": col_match.group(2)
                    })
                
                # å…³ç³»
                relationships = []
                for rel_match in relationship_pattern.finditer(class_body):
                    relationships.append({
                        "field": rel_match.group(1),
                        "target": rel_match.group(2)
                    })
                
                # å¤–é”®
                foreign_keys = fk_pattern.findall(class_body)
                
                rel_path = os.path.relpath(py_file, project_root)
                
                models.append({
                    "model_name": model_name,
                    "table_name": tablename,
                    "file": rel_path,
                    "columns": columns,
                    "relationships": relationships,
                    "foreign_keys": foreign_keys,
                    "column_count": len(columns)
                })
                
                if rel_path not in model_files:
                    model_files.append(rel_path)
    
    print(f"  å‘ç° {len(models)} ä¸ªæ¨¡å‹ï¼Œåˆ†å¸ƒåœ¨ {len(model_files)} ä¸ªæ–‡ä»¶ä¸­")
    
    return {
        "extraction_time": TIMESTAMP,
        "summary": {
            "total_models": len(models),
            "total_files": len(model_files),
            "model_files": model_files
        },
        "models": sorted(models, key=lambda x: x["model_name"])
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æå–å™¨ 2: API ç«¯ç‚¹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_api_endpoints(project_root):
    """
    ä¸¤ç§æ–¹å¼æå–APIç«¯ç‚¹:
    æ–¹å¼A: ä»è¿è¡Œä¸­çš„å¹³å°è·å– OpenAPI spec (ä¼˜å…ˆ)
    æ–¹å¼B: ä»ä»£ç ä¸­æ‰«æ FastAPI router å®šä¹‰
    """
    print("\nğŸ”Œ [2/6] æå–APIç«¯ç‚¹...")
    
    endpoints = []
    
    # === æ–¹å¼B: ä»ä»£ç æ‰«æ ===
    # FastAPI è·¯ç”±æ¨¡å¼
    route_patterns = [
        # @router.get("/path")
        re.compile(r'@(?:router|app)\.(get|post|put|patch|delete)\s*\(\s*["\']([^"\']+)["\']'),
        # @router.api_route("/path", methods=["GET"])
        re.compile(r'@(?:router|app)\.api_route\s*\(\s*["\']([^"\']+)["\'].*methods\s*=\s*\[([^\]]+)\]'),
    ]
    
    # æƒé™ä¾èµ–æ¨¡å¼
    auth_patterns = [
        re.compile(r'Depends\s*\(\s*(require_admin|require_coach_or_admin|get_current_user|require_\w+)\s*\)'),
        re.compile(r'dependencies\s*=\s*\[.*?Depends\s*\(\s*(\w+)\s*\)'),
    ]
    
    # prefix æ¨¡å¼
    prefix_pattern = re.compile(r'(?:APIRouter|router)\s*\(\s*(?:prefix\s*=\s*)?["\']([^"\']*)["\']')
    include_pattern = re.compile(r'include_router\s*\(\s*(\w+).*?prefix\s*=\s*["\']([^"\']+)["\']')
    
    router_files = []
    
    for py_file in glob.glob(str(Path(project_root) / "**" / "*.py"), recursive=True):
        if any(skip in py_file for skip in ["__pycache__", "node_modules", ".venv", "test"]):
            continue
            
        try:
            with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        except:
            continue
        
        if "router" not in content.lower() and "app." not in content:
            continue
            
        # æ‰¾å‰ç¼€
        prefix = ""
        prefix_match = prefix_pattern.search(content)
        if prefix_match:
            prefix = prefix_match.group(1)
        
        found_routes = False
        for pattern in route_patterns:
            for match in pattern.finditer(content):
                groups = match.groups()
                if len(groups) == 2:
                    method, path = groups[0].upper(), groups[1]
                else:
                    path = groups[0]
                    methods_str = groups[1] if len(groups) > 1 else "GET"
                    method = methods_str.replace('"', '').replace("'", "").strip()
                
                # æ‰¾è¯¥ç«¯ç‚¹é™„è¿‘çš„æƒé™è¦æ±‚
                # å¾€å›çœ‹50è¡Œæ‰¾å‡½æ•°å®šä¹‰å’Œä¾èµ–
                line_start = content[:match.start()].rfind('\n', 0, max(0, match.start()-2000))
                context = content[max(0,line_start):match.end()+500]
                
                auth = "unknown"
                for auth_pat in auth_patterns:
                    auth_match = auth_pat.search(context)
                    if auth_match:
                        auth = auth_match.group(1)
                        break
                
                # æ‰¾å‡½æ•°å
                func_match = re.search(r'(?:async\s+)?def\s+(\w+)', content[match.end():match.end()+200])
                func_name = func_match.group(1) if func_match else "unknown"
                
                full_path = prefix + path if not path.startswith(prefix) else path
                
                rel_path = os.path.relpath(py_file, project_root)
                
                endpoints.append({
                    "method": method,
                    "path": full_path,
                    "function": func_name,
                    "auth": auth,
                    "file": rel_path,
                })
                found_routes = True
        
        if found_routes:
            router_files.append(os.path.relpath(py_file, project_root))
    
    # æŒ‰æ¨¡å—åˆ†ç»„
    modules = defaultdict(list)
    for ep in endpoints:
        # ä»è·¯å¾„æå–æ¨¡å—å: /v1/coach/messages -> coach
        parts = ep["path"].strip("/").split("/")
        module = parts[1] if len(parts) > 1 else parts[0] if parts else "root"
        modules[module].append(ep)
    
    print(f"  å‘ç° {len(endpoints)} ä¸ªç«¯ç‚¹ï¼Œ{len(modules)} ä¸ªæ¨¡å—ï¼Œ{len(router_files)} ä¸ªè·¯ç”±æ–‡ä»¶")
    
    return {
        "extraction_time": TIMESTAMP,
        "summary": {
            "total_endpoints": len(endpoints),
            "total_modules": len(modules),
            "router_files": sorted(router_files),
            "by_method": {
                method: len([e for e in endpoints if e["method"] == method])
                for method in sorted(set(e["method"] for e in endpoints))
            },
            "by_auth": {
                auth: len([e for e in endpoints if e["auth"] == auth])
                for auth in sorted(set(e["auth"] for e in endpoints))
            }
        },
        "modules": {k: sorted(v, key=lambda x: x["path"]) for k, v in sorted(modules.items())},
        "all_endpoints": sorted(endpoints, key=lambda x: (x["path"], x["method"]))
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æå–å™¨ 3: Agent æ³¨å†Œè¡¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_agent_registry(project_root):
    """
    æå–æ‰€æœ‰Agentç›¸å…³å®šä¹‰:
    - Agentç±»å®šä¹‰ (domain, keywords, priority, risk_level)
    - AGENT_DOMAIN_MAP é…ç½®
    - è·¯ç”±è§„åˆ™
    - å®‰å…¨å…³é”®è¯
    """
    print("\nğŸ¤– [3/6] æå–Agentæ³¨å†Œè¡¨...")
    
    agents = []
    domain_maps = {}
    router_rules = []
    agent_files = []
    
    # æœç´¢Agentç±»å®šä¹‰
    agent_class_pattern = re.compile(
        r'class\s+(\w*Agent\w*)\s*\(\s*(\w+)\s*\)'
    )
    
    # æœç´¢domainå®šä¹‰
    domain_pattern = re.compile(r'domain\s*=\s*(?:AgentDomain\.)?(\w+)')
    keywords_pattern = re.compile(r'keywords\s*=\s*\[(.*?)\]', re.DOTALL)
    priority_pattern = re.compile(r'priority\s*=\s*(\d+)')
    weight_pattern = re.compile(r'base_weight\s*=\s*([\d.]+)')
    display_name_pattern = re.compile(r'display_name\s*=\s*["\']([^"\']+)["\']')
    
    # æœç´¢ AGENT_DOMAIN_MAP
    domain_map_pattern = re.compile(
        r'AGENT_DOMAIN_MAP\s*[=:]\s*\{(.*?)\}', re.DOTALL
    )
    
    for py_file in glob.glob(str(Path(project_root) / "**" / "*.py"), recursive=True):
        if any(skip in py_file for skip in ["__pycache__", "node_modules", ".venv"]):
            continue
            
        try:
            with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        except:
            continue
        
        rel_path = os.path.relpath(py_file, project_root)
        
        # Agentç±»
        for match in agent_class_pattern.finditer(content):
            class_name = match.group(1)
            base_class = match.group(2)
            
            # è·³è¿‡æ˜æ˜¾ä¸æ˜¯Agentçš„ç±»
            if class_name in ["BaseAgent", "AbstractAgent"]:
                continue
            
            # æå–ç±»ä½“
            class_start = match.start()
            next_class = re.search(r'\nclass\s+', content[class_start+10:])
            class_end = class_start + 10 + next_class.start() if next_class else len(content)
            class_body = content[class_start:class_end]
            
            domain_match = domain_pattern.search(class_body)
            keywords_match = keywords_pattern.search(class_body)
            priority_match = priority_pattern.search(class_body)
            weight_match = weight_pattern.search(class_body)
            display_match = display_name_pattern.search(class_body)
            
            # è§£ækeywordsåˆ—è¡¨
            kw_list = []
            if keywords_match:
                kw_raw = keywords_match.group(1)
                kw_list = re.findall(r'["\']([^"\']+)["\']', kw_raw)
            
            agents.append({
                "class_name": class_name,
                "base_class": base_class,
                "domain": domain_match.group(1) if domain_match else "unknown",
                "display_name": display_match.group(1) if display_match else class_name,
                "keywords": kw_list,
                "priority": int(priority_match.group(1)) if priority_match else None,
                "base_weight": float(weight_match.group(1)) if weight_match else None,
                "file": rel_path,
            })
            
            if rel_path not in agent_files:
                agent_files.append(rel_path)
        
        # AGENT_DOMAIN_MAP
        for map_match in domain_map_pattern.finditer(content):
            map_body = map_match.group(1)
            for line in map_body.split("\n"):
                kv = re.match(r'\s*["\'](\w+)["\']\s*:\s*\[(.*?)\]', line)
                if kv:
                    domain = kv.group(1)
                    related = re.findall(r'["\'](\w+)["\']', kv.group(2))
                    domain_maps[domain] = related
    
    # æœç´¢ AgentDomain æšä¸¾å®šä¹‰
    enum_values = []
    for py_file in glob.glob(str(Path(project_root) / "**" / "*.py"), recursive=True):
        if "__pycache__" in py_file:
            continue
        try:
            with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        except:
            continue
        
        enum_match = re.search(r'class\s+AgentDomain\s*\(.*?\):(.*?)(?=\nclass|\Z)', content, re.DOTALL)
        if enum_match:
            for val_match in re.finditer(r'(\w+)\s*=\s*["\'](\w+)["\']', enum_match.group(1)):
                enum_values.append({
                    "enum_name": val_match.group(1),
                    "value": val_match.group(2)
                })
    
    print(f"  å‘ç° {len(agents)} ä¸ªAgentç±»ï¼Œ{len(domain_maps)} ä¸ªé¢†åŸŸæ˜ å°„ï¼Œ{len(enum_values)} ä¸ªæšä¸¾å€¼")
    
    return {
        "extraction_time": TIMESTAMP,
        "summary": {
            "total_agents": len(agents),
            "total_domains": len(domain_maps),
            "agent_files": sorted(agent_files),
            "enum_values": enum_values,
        },
        "agents": sorted(agents, key=lambda x: x.get("priority") or 99),
        "domain_map": domain_maps,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æå–å™¨ 4: é…ç½®æ–‡ä»¶
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_config_files(project_root):
    """
    æå–æ‰€æœ‰JSON/YAMLé…ç½®æ–‡ä»¶çš„å†…å®¹
    é‡ç‚¹: point_events, milestones, badges, credit_requirements, é—¨æˆ·é…ç½®ç­‰
    """
    print("\nâš™ï¸ [4/6] æå–é…ç½®æ–‡ä»¶...")
    
    configs = {}
    
    # æœç´¢å…³é”®é…ç½®æ–‡ä»¶
    config_patterns = [
        "**/*point*event*.json",
        "**/*milestone*.json",
        "**/*badge*.json",
        "**/*credit*.json",
        "**/*portal*.json",
        "**/*portal*.py",   # æœ‰æ—¶é…ç½®åœ¨py dicté‡Œ
        "**/*config*.json",
        "**/*config*.yaml",
        "**/*config*.yml",
        "**/*settings*.json",
        "**/*settings*.yaml",
        "**/configs/**/*.json",
        "**/config/**/*.json",
        "**/.env.example",
    ]
    
    found_files = set()
    for pattern in config_patterns:
        for f in glob.glob(str(Path(project_root) / pattern), recursive=True):
            if any(skip in f for skip in ["__pycache__", "node_modules", ".venv", "package"]):
                continue
            found_files.add(f)
    
    for filepath in sorted(found_files):
        rel_path = os.path.relpath(filepath, project_root)
        try:
            with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
            
            # å°è¯•è§£æJSON
            if filepath.endswith(".json"):
                try:
                    data = json.loads(content)
                    configs[rel_path] = {
                        "type": "json",
                        "size_bytes": len(content),
                        "content": data
                    }
                except json.JSONDecodeError:
                    configs[rel_path] = {
                        "type": "json_invalid",
                        "size_bytes": len(content),
                        "preview": content[:500]
                    }
            elif filepath.endswith((".yaml", ".yml")):
                configs[rel_path] = {
                    "type": "yaml",
                    "size_bytes": len(content),
                    "content_raw": content[:3000]  # YAML åŸæ ·ä¿ç•™
                }
            else:
                configs[rel_path] = {
                    "type": "text",
                    "size_bytes": len(content),
                    "preview": content[:2000]
                }
        except Exception as e:
            configs[rel_path] = {"type": "error", "error": str(e)}
    
    # é¢å¤–: æ‰«æPythonæ–‡ä»¶ä¸­çš„å†…è”é…ç½®å­—å…¸
    inline_configs = {}
    important_dicts = [
        "PORTAL_CONFIGS", "POINT_EVENTS", "MILESTONES", "BADGES",
        "CREDIT_REQUIREMENTS", "ROLE_PERMISSIONS", "PROMOTION_THRESHOLDS",
        "SERVICE_TIERS", "AGENT_PRIORITIES", "SAFETY_RULES",
    ]
    
    for py_file in glob.glob(str(Path(project_root) / "**" / "*.py"), recursive=True):
        if any(skip in py_file for skip in ["__pycache__", "node_modules", ".venv"]):
            continue
        try:
            with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        except:
            continue
        
        for dict_name in important_dicts:
            pattern = re.compile(rf'{dict_name}\s*[=:]\s*\{{', re.IGNORECASE)
            match = pattern.search(content)
            if match:
                # å°è¯•æå–å­—å…¸èŒƒå›´(ç®€åŒ–ï¼šå–å2000å­—ç¬¦)
                start = match.start()
                snippet = content[start:start+3000]
                rel_path = os.path.relpath(py_file, project_root)
                inline_configs[f"{rel_path}::{dict_name}"] = {
                    "file": rel_path,
                    "variable": dict_name,
                    "preview": snippet[:2000]
                }
    
    print(f"  å‘ç° {len(configs)} ä¸ªé…ç½®æ–‡ä»¶ï¼Œ{len(inline_configs)} ä¸ªå†…è”é…ç½®å­—å…¸")
    
    return {
        "extraction_time": TIMESTAMP,
        "summary": {
            "total_config_files": len(configs),
            "total_inline_configs": len(inline_configs),
            "config_files": sorted(configs.keys()),
            "inline_config_locations": sorted(inline_configs.keys()),
        },
        "config_files": configs,
        "inline_configs": inline_configs,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æå–å™¨ 5: å¤šç§Ÿæˆ·æ¶æ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_tenant_architecture(project_root):
    """
    æå–å¤šç§Ÿæˆ·ç›¸å…³çš„ä»£ç ç»“æ„:
    - Tenantæ¨¡å‹å®šä¹‰
    - ç§Ÿæˆ·éš”ç¦»é€»è¾‘
    - RBAC è§’è‰²å®šä¹‰
    """
    print("\nğŸ¢ [5/6] æå–å¤šç§Ÿæˆ·æ¶æ„...")
    
    tenant_info = {
        "models": [],
        "isolation_patterns": [],
        "rbac_definitions": [],
        "tenant_files": [],
    }
    
    # æœç´¢ tenant ç›¸å…³æ–‡ä»¶
    for py_file in glob.glob(str(Path(project_root) / "**" / "*.py"), recursive=True):
        if any(skip in py_file for skip in ["__pycache__", "node_modules", ".venv"]):
            continue
            
        try:
            with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        except:
            continue
        
        rel_path = os.path.relpath(py_file, project_root)
        
        # tenant_id è¿‡æ»¤æ¨¡å¼
        tenant_filter = re.findall(r'tenant_id\s*==?\s*[:\w.]+', content)
        if tenant_filter:
            tenant_info["isolation_patterns"].append({
                "file": rel_path,
                "patterns": list(set(tenant_filter))[:10]
            })
        
        # RBAC è§’è‰²å®šä¹‰
        role_patterns = [
            re.compile(r'class\s+(\w*Role\w*)\s*\('),
            re.compile(r'(?:ROLES|ROLE_LEVELS|ROLE_PERMISSIONS)\s*=\s*\{'),
            re.compile(r'role_level\s*[><=]+\s*\d+'),
        ]
        
        for rp in role_patterns:
            matches = rp.findall(content)
            if matches:
                tenant_info["rbac_definitions"].append({
                    "file": rel_path,
                    "matches": matches[:10] if isinstance(matches[0], str) else [str(m) for m in matches[:10]]
                })
        
        # Tenant æ¨¡å‹
        if re.search(r'class\s+\w*[Tt]enant\w*\s*\(', content):
            tenant_info["tenant_files"].append(rel_path)
            # æå–è¯¥æ–‡ä»¶çš„å®Œæ•´å†…å®¹(é€šå¸¸ä¸å¤§)
            tenant_info["models"].append({
                "file": rel_path,
                "content": content[:5000]
            })
    
    # æœç´¢æƒé™æ£€æŸ¥å‡½æ•°
    auth_functions = []
    for py_file in glob.glob(str(Path(project_root) / "**" / "*.py"), recursive=True):
        if "__pycache__" in py_file:
            continue
        try:
            with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        except:
            continue
        
        for match in re.finditer(r'(?:async\s+)?def\s+(require_\w+|check_\w*perm\w*|get_current_user)\s*\(', content):
            func_name = match.group(1)
            # æå–å‡½æ•°ä½“(ç®€åŒ–)
            func_start = match.start()
            func_body = content[func_start:func_start+1000]
            auth_functions.append({
                "function": func_name,
                "file": os.path.relpath(py_file, project_root),
                "preview": func_body[:500]
            })
    
    tenant_info["auth_functions"] = auth_functions
    
    print(f"  å‘ç° {len(tenant_info['tenant_files'])} ä¸ªç§Ÿæˆ·æ–‡ä»¶ï¼Œ"
          f"{len(tenant_info['isolation_patterns'])} ä¸ªéš”ç¦»æ¨¡å¼ï¼Œ"
          f"{len(auth_functions)} ä¸ªæƒé™å‡½æ•°")
    
    return {
        "extraction_time": TIMESTAMP,
        "summary": {
            "tenant_files": tenant_info["tenant_files"],
            "isolation_pattern_count": len(tenant_info["isolation_patterns"]),
            "auth_function_count": len(auth_functions),
        },
        **tenant_info
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æå–å™¨ 6: å®‰å…¨ç®¡é“
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_safety_pipeline(project_root):
    """
    æå–å®‰å…¨ç®¡é“ç›¸å…³ä»£ç :
    - Safety Pipeline è§„åˆ™
    - PolicyGate è§„åˆ™
    - å±æœºå¹²é¢„å…³é”®è¯
    - å†…å®¹å®¡æ ¸é€»è¾‘
    """
    print("\nğŸ›¡ï¸ [6/6] æå–å®‰å…¨ç®¡é“...")
    
    safety_info = {
        "pipeline_files": [],
        "crisis_keywords": [],
        "policy_rules": [],
        "safety_configs": [],
    }
    
    safety_keywords = [
        "safety", "crisis", "policy_gate", "policygate", "content_filter",
        "risk_level", "RiskLevel", "CRISIS", "intervention", "escalat"
    ]
    
    for py_file in glob.glob(str(Path(project_root) / "**" / "*.py"), recursive=True):
        if any(skip in py_file for skip in ["__pycache__", "node_modules", ".venv"]):
            continue
            
        try:
            with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        except:
            continue
        
        rel_path = os.path.relpath(py_file, project_root)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å®‰å…¨ç›¸å…³å†…å®¹
        relevance_score = sum(1 for kw in safety_keywords if kw.lower() in content.lower())
        
        if relevance_score >= 2:
            safety_info["pipeline_files"].append({
                "file": rel_path,
                "relevance_score": relevance_score,
                "size": len(content),
            })
            
            # æå–å±æœºå…³é”®è¯åˆ—è¡¨
            kw_matches = re.findall(
                r'(?:CRISIS_KEYWORDS|CRITICAL_KW|WARNING_KW|å±æœº|è‡ªæ€|è‡ªæ®‹)\s*=\s*\[(.*?)\]',
                content, re.DOTALL
            )
            for km in kw_matches:
                keywords = re.findall(r'["\']([^"\']+)["\']', km)
                if keywords:
                    safety_info["crisis_keywords"].extend(keywords)
            
            # æå–PolicyGate/Safetyè§„åˆ™
            rule_matches = re.findall(
                r'(?:POLICY_RULES|SAFETY_RULES|GATE_RULES)\s*=\s*[\[{](.*?)[\]}]',
                content, re.DOTALL
            )
            for rm in rule_matches:
                safety_info["policy_rules"].append({
                    "file": rel_path,
                    "content": rm[:2000]
                })
            
            # RiskLevel æšä¸¾
            risk_enum = re.search(r'class\s+RiskLevel\s*\(.*?\):(.*?)(?=\nclass|\Z)', content, re.DOTALL)
            if risk_enum:
                safety_info["risk_levels"] = re.findall(
                    r'(\w+)\s*=\s*["\']?(\w+)["\']?',
                    risk_enum.group(1)
                )
    
    # å»é‡å…³é”®è¯
    safety_info["crisis_keywords"] = sorted(set(safety_info["crisis_keywords"]))
    
    # æŒ‰ç›¸å…³åº¦æ’åº
    safety_info["pipeline_files"] = sorted(
        safety_info["pipeline_files"],
        key=lambda x: x["relevance_score"],
        reverse=True
    )
    
    print(f"  å‘ç° {len(safety_info['pipeline_files'])} ä¸ªå®‰å…¨ç›¸å…³æ–‡ä»¶ï¼Œ"
          f"{len(safety_info['crisis_keywords'])} ä¸ªå±æœºå…³é”®è¯")
    
    return {
        "extraction_time": TIMESTAMP,
        "summary": {
            "total_safety_files": len(safety_info["pipeline_files"]),
            "crisis_keyword_count": len(safety_info["crisis_keywords"]),
            "policy_rule_count": len(safety_info["policy_rules"]),
        },
        **safety_info
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é¢å¤–: è¿è¡Œæ—¶æå– (å¦‚æœå¹³å°æ­£åœ¨è¿è¡Œ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_runtime_info(base_url="http://localhost:8000"):
    """
    ä»è¿è¡Œä¸­çš„å¹³å°æå–è¿è¡Œæ—¶ä¿¡æ¯(å¯é€‰)
    """
    print(f"\nğŸŒ [é™„åŠ ] å°è¯•ä»è¿è¡Œä¸­çš„å¹³å°æå– ({base_url})...")
    
    runtime = {"available": False}
    
    try:
        import requests
        
        # OpenAPI spec
        try:
            resp = requests.get(f"{base_url}/openapi.json", timeout=5)
            if resp.status_code == 200:
                spec = resp.json()
                runtime["openapi"] = {
                    "title": spec.get("info", {}).get("title"),
                    "version": spec.get("info", {}).get("version"),
                    "total_paths": len(spec.get("paths", {})),
                    "paths": list(spec.get("paths", {}).keys()),
                }
                runtime["available"] = True
                print(f"  âœ… OpenAPI: {runtime['openapi']['total_paths']} ä¸ªè·¯å¾„")
        except:
            print(f"  âš ï¸ æ— æ³•è·å– OpenAPI spec")
        
        # å¥åº·æ£€æŸ¥
        for health_path in ["/health", "/api/health", "/api/v1/health"]:
            try:
                resp = requests.get(f"{base_url}{health_path}", timeout=3)
                if resp.status_code == 200:
                    runtime["health"] = resp.json()
                    print(f"  âœ… å¥åº·æ£€æŸ¥: {health_path}")
                    break
            except:
                continue
        
    except ImportError:
        print(f"  âš ï¸ requests æœªå®‰è£…ï¼Œè·³è¿‡è¿è¡Œæ—¶æå– (pip install requests)")
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•è¿æ¥å¹³å°: {e}")
    
    return runtime


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é¡¹ç›®ç»“æ„æ¦‚è§ˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_project_structure(project_root, max_depth=3):
    """æå–é¡¹ç›®ç›®å½•ç»“æ„"""
    print("\nğŸ“ [é™„åŠ ] æå–é¡¹ç›®ç»“æ„...")
    
    structure = {"dirs": [], "py_files": 0, "js_files": 0, "json_files": 0, "total_files": 0}
    
    skip_dirs = {"__pycache__", "node_modules", ".venv", "venv", ".git", 
                 ".next", "dist", "build", ".tox", ".pytest_cache"}
    
    for root, dirs, files in os.walk(project_root):
        # è¿‡æ»¤
        dirs[:] = [d for d in dirs if d not in skip_dirs and not d.startswith(".")]
        
        depth = root.replace(str(project_root), "").count(os.sep)
        if depth > max_depth:
            dirs.clear()
            continue
        
        rel_root = os.path.relpath(root, project_root)
        if rel_root == ".":
            rel_root = ""
        
        file_summary = {}
        for f in files:
            ext = Path(f).suffix.lower()
            file_summary[ext] = file_summary.get(ext, 0) + 1
            structure["total_files"] += 1
            if ext == ".py": structure["py_files"] += 1
            elif ext in (".js", ".jsx", ".ts", ".tsx", ".vue"): structure["js_files"] += 1
            elif ext == ".json": structure["json_files"] += 1
        
        if file_summary:
            structure["dirs"].append({
                "path": rel_root or ".",
                "depth": depth,
                "subdirs": [d for d in dirs],
                "files": file_summary,
            })
    
    print(f"  é¡¹ç›®å…± {structure['total_files']} ä¸ªæ–‡ä»¶ "
          f"(Python: {structure['py_files']}, "
          f"JS/Vue: {structure['js_files']}, "
          f"JSON: {structure['json_files']})")
    
    return structure


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»ç¨‹åº
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description="è¡Œå¥å¹³å°å¥‘çº¦æ³¨å†Œè¡¨ä¿¡æ¯æå–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python extract_platform_info.py --project-root D:\\behavioral-health-project
  python extract_platform_info.py --project-root D:\\behavioral-health-project --api-url http://localhost:8000
        """
    )
    parser.add_argument("--project-root", "-p", required=True, help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--api-url", "-u", default=None, help="è¿è¡Œä¸­å¹³å°çš„URL(å¯é€‰)")
    parser.add_argument("--skip-runtime", action="store_true", help="è·³è¿‡è¿è¡Œæ—¶æå–")
    
    args = parser.parse_args()
    project_root = Path(args.project_root).resolve()
    
    if not project_root.exists():
        print(f"âŒ é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {project_root}")
        sys.exit(1)
    
    print("=" * 60)
    print("  è¡Œå¥å¹³å° â€” å¥‘çº¦æ³¨å†Œè¡¨ä¿¡æ¯æå–å·¥å…·")
    print(f"  é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"  æå–æ—¶é—´: {TIMESTAMP}")
    print("=" * 60)
    
    out_dir = ensure_output_dir(project_root)
    print(f"\nè¾“å‡ºç›®å½•: {out_dir}")
    
    # é¡¹ç›®ç»“æ„
    structure = extract_project_structure(project_root)
    save_json(out_dir, "0_project_structure.json", structure)
    
    # 6 ç±»æå–
    models = extract_data_models(project_root)
    save_json(out_dir, "1_data_models.json", models)
    
    endpoints = extract_api_endpoints(project_root)
    save_json(out_dir, "2_api_endpoints.json", endpoints)
    
    agents = extract_agent_registry(project_root)
    save_json(out_dir, "3_agent_registry.json", agents)
    
    configs = extract_config_files(project_root)
    save_json(out_dir, "4_config_files.json", configs)
    
    tenant = extract_tenant_architecture(project_root)
    save_json(out_dir, "5_tenant_architecture.json", tenant)
    
    safety = extract_safety_pipeline(project_root)
    save_json(out_dir, "6_safety_pipeline.json", safety)
    
    # è¿è¡Œæ—¶(å¯é€‰)
    if not args.skip_runtime:
        api_url = args.api_url or "http://localhost:8000"
        runtime = extract_runtime_info(api_url)
        save_json(out_dir, "7_runtime_info.json", runtime)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary = {
        "extraction_time": TIMESTAMP,
        "project_root": str(project_root),
        "totals": {
            "data_models": models["summary"]["total_models"],
            "api_endpoints": endpoints["summary"]["total_endpoints"],
            "api_modules": endpoints["summary"]["total_modules"],
            "agents": agents["summary"]["total_agents"],
            "agent_domains": agents["summary"]["total_domains"],
            "config_files": configs["summary"]["total_config_files"],
            "inline_configs": configs["summary"]["total_inline_configs"],
            "tenant_files": len(tenant["summary"]["tenant_files"]),
            "auth_functions": tenant["summary"]["auth_function_count"],
            "safety_files": safety["summary"]["total_safety_files"],
            "crisis_keywords": safety["summary"]["crisis_keyword_count"],
        },
        "output_files": [
            "0_project_structure.json â€” é¡¹ç›®ç›®å½•ç»“æ„",
            "1_data_models.json â€” SQLAlchemy æ•°æ®æ¨¡å‹",
            "2_api_endpoints.json â€” API ç«¯ç‚¹æ¸…å•",
            "3_agent_registry.json â€” Agent æ³¨å†Œè¡¨",
            "4_config_files.json â€” é…ç½®æ–‡ä»¶å†…å®¹",
            "5_tenant_architecture.json â€” å¤šç§Ÿæˆ·æ¶æ„",
            "6_safety_pipeline.json â€” å®‰å…¨ç®¡é“",
            "7_runtime_info.json â€” è¿è¡Œæ—¶ä¿¡æ¯(å¦‚æœå¯ç”¨)",
        ],
        "next_steps": [
            "å°† _contract_extraction/ æ–‡ä»¶å¤¹æ•´ä½“å‘é€ç»™Claude",
            "Claudeå°†åŸºäºè¿™äº›æ•°æ®ç”Ÿæˆå¥‘çº¦æ³¨å†Œè¡¨è‰ç¨¿",
            "æ ‡æ³¨ã€Œå·²ç¡®è®¤ã€ã€Œå¾…ç¡®è®¤ã€ã€Œå¾…å†³ç­–ã€ä¸‰ç§çŠ¶æ€",
            "å›¢é˜ŸReviewåé€æ­¥ç¡®è®¤ï¼Œå½¢æˆæ­£å¼ç‰ˆå¥‘çº¦æ³¨å†Œè¡¨",
        ]
    }
    save_json(out_dir, "SUMMARY.json", summary)
    
    # æ‰“å°æ±‡æ€»
    print("\n" + "=" * 60)
    print("  âœ… æå–å®Œæˆï¼æ±‡æ€»å¦‚ä¸‹ï¼š")
    print("=" * 60)
    for k, v in summary["totals"].items():
        print(f"  {k}: {v}")
    print(f"\n  ğŸ“‚ è¾“å‡ºç›®å½•: {out_dir}")
    print(f"  ğŸ“„ å…± {len(list(out_dir.glob('*.json')))} ä¸ªæ–‡ä»¶")
    print("\n  ä¸‹ä¸€æ­¥: å°†æ•´ä¸ª _contract_extraction/ æ–‡ä»¶å¤¹çš„å†…å®¹")
    print("  ä¸Šä¼ åˆ°Claudeå¯¹è¯ä¸­ï¼Œå³å¯ç”Ÿæˆå¥‘çº¦æ³¨å†Œè¡¨è‰ç¨¿ã€‚")
    print("=" * 60)


if __name__ == "__main__":
    main()
