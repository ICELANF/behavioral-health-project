"""
Coach Agent â€” å°†è¯Šæ–­ç®¡é“ + RAG + LLM æ•´åˆä¸ºç»Ÿä¸€å¯¹è¯ä»£ç†
æ”¾ç½®: api/core/llm/coach_agent.py

èŒè´£:
  1. æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯, åˆ¤æ–­æ„å›¾
  2. æŸ¥ RAG çŸ¥è¯†åº“è·å–å‚è€ƒ
  3. ç»“åˆç”¨æˆ·ç”»åƒ (DiagnosticPipeline è¾“å‡º) ç”Ÿæˆå›å¤
  4. è®°å½•å¯¹è¯æ—¥å¿—ä¾›æ•ˆæœè¿½è¸ªä½¿ç”¨

ä¸ç°æœ‰æ¨¡å—çš„å…³ç³»:
  - è¯»å–: DiagnosticPipeline.PipelineResult (ç”¨æˆ·ç”»åƒ)
  - è¯»å–: InterventionStrategyEngine.match()  (ç­–ç•¥è¯æœ¯)
  - è¯»å–: IncentiveIntegration (æˆé•¿ç­‰çº§/å¤„æ–¹æ¨¡å¼)
  - å†™å…¥: llm_call_logs è¡¨ (è°ƒç”¨æ—¥å¿—)
  - è°ƒç”¨: RAGPipeline (çŸ¥è¯†æ£€ç´¢+ç”Ÿæˆ)
  - è°ƒç”¨: LLMRouter (ä¸‰çº§è·¯ç”±)
"""
import json
import logging
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

from core.llm.client import LLMClient, LLMResponse
from core.llm.router import LLMRouter, TaskComplexity, INTENT_COMPLEXITY
from core.rag.pipeline import RAGPipeline, RAGResult, RAGConfig
from core.rag.vector_store import QdrantStore

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ„å›¾è¯†åˆ« Prompt
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INTENT_CLASSIFY_PROMPT = """ä½ æ˜¯ BHP è¡Œä¸ºå¥åº·å¹³å°çš„æ„å›¾åˆ†ç±»å™¨ã€‚
è¯·åˆ¤æ–­ç”¨æˆ·æ¶ˆæ¯çš„æ„å›¾ç±»åˆ«, ä»…è¾“å‡ºä¸€ä¸ªæ ‡ç­¾ã€‚

å¯é€‰æ ‡ç­¾:
- greeting: æ‰“æ‹›å‘¼/é—®å€™
- checkin_confirm: æ‰“å¡/ç­¾åˆ°ç¡®è®¤
- mood_tag: è¡¨è¾¾æƒ…ç»ª/å¿ƒæƒ…
- knowledge_qa: çŸ¥è¯†/æ¦‚å¿µç±»æé—®
- task_reminder: è¯¢é—®ä»»åŠ¡/è¿›åº¦
- progress_summary: æŸ¥çœ‹æ±‡æ€»/æŠ¥å‘Š
- strategy_explain: è¯¢é—®å¹²é¢„ç­–ç•¥çš„åŸå› 
- coach_dialogue: å¯»æ±‚æ·±åº¦æŒ‡å¯¼/å›°æƒ‘
- stage_assessment: è¦æ±‚è¯„ä¼°/æµ‹è¯„
- crisis_support: è¡¨è¾¾ä¸¥é‡å›°æ‰°/å±æœº
- general_chat: é—²èŠ/å…¶ä»–

ç”¨æˆ·æ¶ˆæ¯: {message}
æ„å›¾æ ‡ç­¾:"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å¯¹è¯ä¸Šä¸‹æ–‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class UserContext:
    """ç”¨æˆ·ä¸Šä¸‹æ–‡ (ä» PipelineResult æå–)"""
    user_id: int
    behavioral_stage: str = "S0"
    readiness_level: str = "L1"
    spi_score: float = 0.0
    bpt_type: str = "mixed"
    cultivation_stage: str = "startup"
    growth_level: str = "G0"
    streak_days: int = 0
    health_competency: str = "Lv0"
    top_obstacles: list = field(default_factory=list)
    dominant_causes: list = field(default_factory=list)
    strengths: list = field(default_factory=list)
    weaknesses: list = field(default_factory=list)
    support_level: str = "moderate"

    def to_profile_dict(self) -> dict:
        """è½¬ä¸º RAGPipeline éœ€è¦çš„ profile å­—å…¸"""
        return {
            "behavioral_stage": self.behavioral_stage,
            "readiness_level": self.readiness_level,
            "spi_score": self.spi_score,
            "behavior_type": self.bpt_type,
            "cultivation_stage": self.cultivation_stage,
            "growth_level": self.growth_level,
            "health_competency": self.health_competency,
            "top_obstacles": self.top_obstacles,
            "dominant_causes": self.dominant_causes,
            "support_level": self.support_level,
        }

    @classmethod
    def from_pipeline_result(cls, pr) -> "UserContext":
        """ä» DiagnosticPipeline.PipelineResult æ„å»º"""
        ctx = cls(user_id=pr.user_id)
        ctx.behavioral_stage = pr.layer1.behavioral_stage
        ctx.bpt_type = pr.layer1.bpt_type
        ctx.spi_score = pr.layer2.spi_score
        ctx.readiness_level = pr.layer2.readiness_level

        if pr.layer3:
            ctx.strengths = pr.layer3.strengths
            ctx.weaknesses = pr.layer3.weaknesses
            ctx.top_obstacles = [
                o.get("category", "unknown")
                for o in pr.layer3.obstacles.get("top_obstacles", [])[:3]
            ] if pr.layer3.obstacles else []
            if pr.layer3.support:
                ctx.support_level = pr.layer3.support.get("support_level", "moderate")

        if pr.layer2.cause_analysis:
            sorted_causes = sorted(
                pr.layer2.cause_analysis.get("scores", {}).items(),
                key=lambda x: x[1], reverse=True,
            )
            ctx.dominant_causes = [c[0] for c in sorted_causes[:3]]

        ctx.cultivation_stage = pr.layer4.cultivation_stage
        ctx.growth_level = pr.layer4.incentive_context.get("growth_level", "G0")

        return ctx


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å¯¹è¯æ—¥å¿— (å†…å­˜ç¼“å­˜, å¯å®šæœŸå†™å…¥æ•°æ®åº“)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class ConversationTurn:
    """å•è½®å¯¹è¯è®°å½•"""
    turn_id: int
    user_id: int
    timestamp: datetime
    user_message: str
    intent: str
    assistant_message: str
    model_used: str
    input_tokens: int = 0
    output_tokens: int = 0
    cost_yuan: float = 0.0
    latency_ms: int = 0
    rag_sources: list = field(default_factory=list)
    used_rag: bool = False

    def to_db_dict(self) -> dict:
        """è½¬ä¸ºæ•°æ®åº“å†™å…¥æ ¼å¼"""
        return {
            "user_id": self.user_id,
            "timestamp": self.timestamp.isoformat(),
            "user_message": self.user_message[:500],
            "intent": self.intent,
            "assistant_message": self.assistant_message[:2000],
            "model_used": self.model_used,
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "cost_yuan": self.cost_yuan,
            "latency_ms": self.latency_ms,
            "rag_source_count": len(self.rag_sources),
            "used_rag": self.used_rag,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Coach Agent
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# éœ€è¦ RAG çš„æ„å›¾
RAG_INTENTS = {
    "knowledge_qa", "strategy_explain", "coach_dialogue",
    "stage_assessment", "crisis_support", "progress_summary",
}

# ç®€å•å›å¤ (ä¸è°ƒ LLM)
QUICK_REPLIES: dict[str, str] = {
    "checkin_confirm": "æ‰“å¡æˆåŠŸï¼ä»Šå¤©çš„ä»»åŠ¡å®Œæˆå¾—å¾ˆå¥½ ğŸ’ª ç»§ç»­ä¿æŒï¼",
}


class CoachAgent:
    """
    BHP å¯¹è¯ä»£ç† â€” ç»Ÿä¸€å…¥å£

    ç”¨æ³•:
        agent = CoachAgent()
        reply = agent.chat(
            user_id=123,
            message="SPIè¯„åˆ†ä½äº30æ˜¯ä¸æ˜¯è¯´æ˜æˆ‘æ²¡æ•‘äº†ï¼Ÿ",
            user_context=ctx,       # ä» DiagnosticPipeline è·å–
            history=prev_messages,  # å¯é€‰: å¤šè½®å¯¹è¯å†å²
        )
        print(reply["answer"])
    """

    def __init__(
        self,
        llm_client: LLMClient | None = None,
        router: LLMRouter | None = None,
        rag_pipeline: RAGPipeline | None = None,
        qdrant_url: str | None = None,
    ):
        self.llm = llm_client or LLMClient()
        self.router = router or LLMRouter(self.llm)

        if rag_pipeline:
            self.rag = rag_pipeline
        else:
            store = QdrantStore(base_url=qdrant_url)
            self.rag = RAGPipeline(self.llm, self.router, store)

        self._turn_counter = 0
        self._conversation_log: list[ConversationTurn] = []

    def chat(
        self,
        user_id: int,
        message: str,
        user_context: UserContext | None = None,
        history: list[dict] | None = None,
        force_intent: str | None = None,
    ) -> dict:
        """
        ä¸»å¯¹è¯æ¥å£

        Args:
            user_id: ç”¨æˆ·ID
            message: ç”¨æˆ·æ¶ˆæ¯
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ (ç”»åƒ)
            history: å¤šè½®å¯¹è¯å†å² [{"role":"user","content":"..."},...]
            force_intent: å¼ºåˆ¶æŒ‡å®šæ„å›¾ (è·³è¿‡åˆ†ç±»)

        Returns:
            {
                "answer": str,
                "intent": str,
                "model": str,
                "sources": list,
                "latency_ms": int,
                "tokens": int,
                "cost_yuan": float,
            }
        """
        t0 = time.time()

        # Step 1: æ„å›¾è¯†åˆ«
        intent = force_intent or self._classify_intent(message)

        # Step 2: å¿«æ·å›å¤æ£€æŸ¥
        if intent in QUICK_REPLIES and not history:
            answer = QUICK_REPLIES[intent]
            latency = int((time.time() - t0) * 1000)
            self._log_turn(user_id, message, intent, answer, "", 0, 0, 0, latency, [])
            return {
                "answer": answer,
                "intent": intent,
                "model": "builtin",
                "sources": [],
                "latency_ms": latency,
                "tokens": 0,
                "cost_yuan": 0,
            }

        # Step 3: åˆ¤æ–­æ˜¯å¦éœ€è¦ RAG
        use_rag = intent in RAG_INTENTS

        # Step 4: ç”Ÿæˆå›å¤
        if use_rag and intent == "coach_dialogue" and user_context:
            # Coach å¯¹è¯: RAG + ç”»åƒ
            result = self.rag.coach_query(
                question=message,
                user_profile=user_context.to_profile_dict(),
                history=history,
            )
        elif use_rag:
            # çŸ¥è¯†é—®ç­”: çº¯ RAG
            result = self.rag.query(
                question=message,
                history=history,
            )
        else:
            # ç®€å•å¯¹è¯: ç›´æ¥ LLM
            system = self._build_simple_system(user_context)
            messages = list(history or [])
            messages.append({"role": "user", "content": message})

            llm_resp = self.router.route(
                messages=messages,
                system=system,
                intent=intent,
            )
            result = RAGResult(
                answer=llm_resp.content,
                sources=[],
                llm_response=llm_resp,
                query=message,
                latency_ms=int((time.time() - t0) * 1000),
            )

        latency = int((time.time() - t0) * 1000)

        # Step 5: è®°å½•æ—¥å¿—
        llm_resp = result.llm_response
        self._log_turn(
            user_id, message, intent, result.answer,
            llm_resp.model if llm_resp else "",
            llm_resp.input_tokens if llm_resp else 0,
            llm_resp.output_tokens if llm_resp else 0,
            llm_resp.cost_yuan if llm_resp else 0,
            latency, result.sources, use_rag,
        )

        return {
            "answer": result.answer,
            "intent": intent,
            "model": llm_resp.model if llm_resp else "builtin",
            "sources": result.sources,
            "latency_ms": latency,
            "tokens": llm_resp.total_tokens if llm_resp else 0,
            "cost_yuan": llm_resp.cost_yuan if llm_resp else 0,
        }

    def generate_prescription(
        self,
        user_context: UserContext,
        layer3_report: dict | None = None,
    ) -> dict:
        """
        RAG å¢å¼ºå¤„æ–¹ç”Ÿæˆ

        å°†å·²æœ‰çš„è§„åˆ™å¼•æ“å¤„æ–¹ (DiagnosticPipeline.Layer4)
        ä¸ LLM ç”Ÿæˆçš„ä¸ªæ€§åŒ–è¯æœ¯/ä»»åŠ¡åç§°ç»“åˆ
        """
        result = self.rag.prescription_query(
            user_profile=user_context.to_profile_dict(),
            layer3_report=layer3_report,
        )

        return {
            "prescription_text": result.answer,
            "sources": result.sources,
            "model": result.llm_response.model if result.llm_response else "",
            "cost_yuan": result.llm_response.cost_yuan if result.llm_response else 0,
        }

    def get_conversation_log(self, user_id: int | None = None) -> list[dict]:
        """è·å–å¯¹è¯æ—¥å¿—"""
        logs = self._conversation_log
        if user_id:
            logs = [l for l in logs if l.user_id == user_id]
        return [l.to_db_dict() for l in logs]

    def get_stats(self) -> dict:
        """æ±‡æ€»ç»Ÿè®¡"""
        router_stats = self.router.get_stats()
        return {
            "total_turns": len(self._conversation_log),
            "router": router_stats,
        }

    # â”€â”€ å†…éƒ¨æ–¹æ³• â”€â”€

    def _classify_intent(self, message: str) -> str:
        """ç”¨è½»é‡æ¨¡å‹åˆ†ç±»æ„å›¾"""
        try:
            prompt = INTENT_CLASSIFY_PROMPT.format(message=message)
            resp = self.router.route(
                messages=[{"role": "user", "content": prompt}],
                complexity=TaskComplexity.SIMPLE,
                max_tokens=20,
                temperature=0.1,
            )
            intent = resp.content.strip().lower().replace('"', "").replace("'", "")
            # éªŒè¯æ˜¯å¦åœ¨å·²çŸ¥æ„å›¾åˆ—è¡¨ä¸­
            if intent in INTENT_COMPLEXITY:
                return intent
            return "general_chat"
        except Exception as e:
            logger.warning(f"Intent classification failed: {e}")
            return "general_chat"

    def _build_simple_system(self, ctx: UserContext | None) -> str:
        """æ„å»ºç®€å•å¯¹è¯çš„ system prompt"""
        base = (
            "ä½ æ˜¯ BHP è¡Œä¸ºå¥åº·ä¿ƒè¿›å¹³å°çš„AIåŠ©æ‰‹ã€‚\n"
            "è¯·ç”¨æ¸©æš–ã€é¼“åŠ±çš„è¯­æ°”ä¸ç”¨æˆ·äº¤æµã€‚\n"
            "å›å¤ç®€æ´, æ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚"
        )
        if ctx:
            base += f"\n\nç”¨æˆ·å½“å‰è¡Œä¸ºé˜¶æ®µ: {ctx.behavioral_stage}"
            if ctx.spi_score:
                base += f", SPIè¯„åˆ†: {ctx.spi_score}"
        return base

    def _log_turn(
        self,
        user_id: int,
        message: str,
        intent: str,
        answer: str,
        model: str,
        input_tokens: int,
        output_tokens: int,
        cost: float,
        latency: int,
        sources: list,
        used_rag: bool = False,
    ):
        self._turn_counter += 1
        turn = ConversationTurn(
            turn_id=self._turn_counter,
            user_id=user_id,
            timestamp=datetime.now(),
            user_message=message,
            intent=intent,
            assistant_message=answer,
            model_used=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            cost_yuan=cost,
            latency_ms=latency,
            rag_sources=sources,
            used_rag=used_rag,
        )
        self._conversation_log.append(turn)

        # åªä¿ç•™æœ€è¿‘ 1000 æ¡ (å†…å­˜ä¿æŠ¤)
        if len(self._conversation_log) > 1000:
            self._conversation_log = self._conversation_log[-500:]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¾¿æ·å·¥å‚
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_coach_agent(qdrant_url: str | None = None) -> CoachAgent:
    """ä¸€ç«™å¼åˆ›å»º CoachAgent"""
    return CoachAgent(qdrant_url=qdrant_url)
