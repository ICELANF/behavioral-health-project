#!/usr/bin/env python3
"""
çŸ¥è¯†åº“å…¥åº“è„šæœ¬

è¯»å– knowledge/ ä¸‹çš„ Markdown æ–‡ä»¶ â†’ åˆ†å— â†’ Ollama åµŒå…¥ â†’ å†™å…¥ DB

ç›®å½•çº¦å®š:
  knowledge/kb_theory/*.md     â†’ scope=platform, domain=general
  knowledge/kb_case_studies/*.md â†’ scope=platform, domain=æŒ‰æ–‡ä»¶åæ¨æ–­
  knowledge/kb_domain/<domain>/*.md â†’ scope=domain, domain_id=<domain>

ç”¨æ³•:
  python scripts/ingest_knowledge.py
  python scripts/ingest_knowledge.py --dir knowledge/kb_theory --scope platform --domain general
"""

import os
import sys
import re
import json
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

from core.database import SessionLocal, engine
from core.models import Base, KnowledgeDocument, KnowledgeChunk
from core.knowledge.embedding_service import EmbeddingService


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åˆ†å—ç­–ç•¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def chunk_markdown(text: str, max_chars: int = 800, overlap: int = 100):
    """
    Markdown åˆ†å—ç­–ç•¥:
      1. å…ˆæŒ‰ ## æ ‡é¢˜åˆ†å‰²æˆé€»è¾‘æ®µ
      2. æ®µå†…å¦‚æœè¶…é•¿ï¼Œå†æŒ‰æ®µè½åˆ†å‰²
      3. æ¯å—ä¿ç•™æ‰€å±æ ‡é¢˜ (heading)
    """
    chunks = []
    current_heading = ""

    # æŒ‰ ## æ ‡é¢˜åˆ†å‰²
    sections = re.split(r'^(#{1,3}\s+.+)$', text, flags=re.MULTILINE)

    buffer = ""
    for section in sections:
        section = section.strip()
        if not section:
            continue

        # æ£€æµ‹æ ‡é¢˜
        if re.match(r'^#{1,3}\s+', section):
            # å…ˆä¿å­˜ä¹‹å‰çš„ buffer
            if buffer.strip():
                for sub in _split_long(buffer.strip(), max_chars, overlap):
                    chunks.append({"heading": current_heading, "content": sub})
                buffer = ""
            current_heading = re.sub(r'^#{1,3}\s+', '', section).strip()
            continue

        buffer += "\n\n" + section

    # æ”¶å°¾
    if buffer.strip():
        for sub in _split_long(buffer.strip(), max_chars, overlap):
            chunks.append({"heading": current_heading, "content": sub})

    return chunks


def _split_long(text: str, max_chars: int, overlap: int):
    """è¶…é•¿æ–‡æœ¬æŒ‰æ®µè½åˆ‡åˆ†"""
    if len(text) <= max_chars:
        return [text]

    paragraphs = text.split("\n\n")
    result = []
    current = ""

    for para in paragraphs:
        if len(current) + len(para) + 2 > max_chars and current:
            result.append(current.strip())
            # ä¿ç•™å°¾éƒ¨ overlap å­—ç¬¦
            current = current[-overlap:] + "\n\n" + para if overlap else para
        else:
            current = (current + "\n\n" + para) if current else para

    if current.strip():
        result.append(current.strip())

    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ–‡ä»¶å…ƒæ•°æ®æ¨æ–­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DOMAIN_KEYWORDS = {
    "nutrition": ["è¥å…»", "é¥®é£Ÿ", "è†³é£Ÿ", "nutrition", "diet", "é£Ÿç‰©"],
    "exercise": ["è¿åŠ¨", "é”»ç‚¼", "exercise", "fitness", "åº·å¤"],
    "sleep": ["ç¡çœ ", "sleep", "å¤±çœ ", "ä½œæ¯"],
    "mental": ["å¿ƒç†", "æƒ…ç»ª", "mental", "ç„¦è™‘", "æŠ‘éƒ", "å‹åŠ›"],
    "glucose": ["è¡€ç³–", "ç³–å°¿", "glucose", "insulin", "èƒ°å²›ç´ "],
    "tcm": ["ä¸­åŒ»", "ä½“è´¨", "ç»ç»œ", "tcm", "å…»ç”Ÿ"],
    "cardiac": ["å¿ƒè„", "å¿ƒè¡€ç®¡", "cardiac", "å† å¿ƒ"],
    "metabolism": ["ä»£è°¢", "metabol", "å†…åˆ†æ³Œ"],
    "behavior": ["è¡Œä¸º", "ä¹ æƒ¯", "behavior", "åŠ¨æœº"],
}


def infer_domain(filename: str, content: str) -> str:
    """ä»æ–‡ä»¶åå’Œå†…å®¹æ¨æ–­é¢†åŸŸ"""
    text = (filename + " " + content[:500]).lower()
    scores = {}
    for domain, keywords in DOMAIN_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw.lower() in text)
        if score > 0:
            scores[domain] = score
    if scores:
        return max(scores, key=scores.get)
    return "general"


def extract_metadata(content: str, filename: str):
    """ä» Markdown æ–‡ä»¶æå–å…ƒæ•°æ®"""
    title = filename.replace(".md", "").replace("_", " ").strip()
    author = ""
    source = ""

    # å°è¯•ä» YAML frontmatter æå–
    fm_match = re.match(r'^---\s*\n(.+?)\n---', content, re.DOTALL)
    if fm_match:
        fm = fm_match.group(1)
        title_m = re.search(r'title:\s*(.+)', fm)
        author_m = re.search(r'author:\s*(.+)', fm)
        source_m = re.search(r'source:\s*(.+)', fm)
        if title_m:
            title = title_m.group(1).strip().strip('"').strip("'")
        if author_m:
            author = author_m.group(1).strip().strip('"').strip("'")
        if source_m:
            source = source_m.group(1).strip().strip('"').strip("'")

    # ä»ç¬¬ä¸€ä¸ª # æ ‡é¢˜æå–
    if not title or title == filename.replace(".md", ""):
        h1 = re.search(r'^#\s+(.+)', content, re.MULTILINE)
        if h1:
            title = h1.group(1).strip()

    return title, author, source


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»å…¥åº“é€»è¾‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def ingest_directory(
    directory: str,
    scope: str = "platform",
    domain_id: str = None,
    tenant_id: str = None,
    priority: int = 5,
):
    """å…¥åº“ä¸€ä¸ªç›®å½•ä¸‹çš„æ‰€æœ‰ Markdown æ–‡ä»¶"""
    if not os.path.isdir(directory):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return

    md_files = [f for f in os.listdir(directory) if f.endswith(".md")]
    if not md_files:
        print(f"âš ï¸ ç›®å½•ä¸‹æ—  .md æ–‡ä»¶: {directory}")
        return

    print(f"\nğŸ“‚ å…¥åº“ç›®å½•: {directory}")
    print(f"   scope={scope}, domain={domain_id or 'auto'}, files={len(md_files)}")

    embedder = EmbeddingService()
    db = SessionLocal()

    total_chunks = 0

    try:
        for filename in sorted(md_files):
            filepath = os.path.join(directory, filename)
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()

            if not content.strip():
                print(f"  â­ è·³è¿‡ç©ºæ–‡ä»¶: {filename}")
                continue

            # å…ƒæ•°æ®
            title, author, source = extract_metadata(content, filename)
            file_domain = domain_id or infer_domain(filename, content)

            # æ£€æŸ¥æ˜¯å¦å·²å…¥åº“
            existing = db.query(KnowledgeDocument).filter(
                KnowledgeDocument.title == title,
                KnowledgeDocument.scope == scope,
            ).first()
            if existing:
                print(f"  â­ å·²å…¥åº“: {title} (id={existing.id})")
                continue

            # åˆ†å—
            chunks = chunk_markdown(content)
            if not chunks:
                print(f"  â­ æ— æœ‰æ•ˆå†…å®¹: {filename}")
                continue

            # åˆ›å»ºæ–‡æ¡£
            doc = KnowledgeDocument(
                title=title,
                author=author,
                source=source,
                domain_id=file_domain,
                scope=scope,
                tenant_id=tenant_id,
                priority=priority,
                is_active=True,
                status="processing",
                file_path=filepath,
                chunk_count=len(chunks),
                created_at=datetime.utcnow(),
            )
            db.add(doc)
            db.flush()  # è·å– doc.id

            # åµŒå…¥ & åˆ›å»º chunks
            texts = [c["content"] for c in chunks]
            print(f"  ğŸ“„ {title} â†’ {len(chunks)} å—, åµŒå…¥ä¸­...")
            embeddings = embedder.embed_batch(texts)

            for i, (chunk_data, embedding) in enumerate(zip(chunks, embeddings)):
                chunk = KnowledgeChunk(
                    document_id=doc.id,
                    content=chunk_data["content"],
                    heading=chunk_data.get("heading", ""),
                    chunk_index=i,
                    doc_title=title,
                    doc_author=author,
                    doc_source=source,
                    scope=scope,
                    domain_id=file_domain,
                    tenant_id=tenant_id,
                    embedding=json.dumps(embedding) if embedding else None,
                    created_at=datetime.utcnow(),
                )
                db.add(chunk)

            doc.status = "ready"
            db.commit()
            total_chunks += len(chunks)
            print(f"  âœ… {title}: {len(chunks)} å—å·²å…¥åº“ (domain={file_domain})")

        print(f"\nâœ… å®Œæˆ! å…±å…¥åº“ {total_chunks} ä¸ªæ–‡æœ¬å—")

    except Exception as e:
        db.rollback()
        print(f"âŒ å…¥åº“å¤±è´¥: {e}")
        raise
    finally:
        embedder.close()
        db.close()


def main():
    parser = argparse.ArgumentParser(description="çŸ¥è¯†åº“å…¥åº“è„šæœ¬")
    parser.add_argument("--dir", type=str, help="æŒ‡å®šå…¥åº“ç›®å½•")
    parser.add_argument("--scope", type=str, default="platform", choices=["platform", "domain", "tenant"])
    parser.add_argument("--domain", type=str, default=None, help="é¢†åŸŸID (ä¸æŒ‡å®šåˆ™è‡ªåŠ¨æ¨æ–­)")
    parser.add_argument("--tenant", type=str, default=None, help="ç§Ÿæˆ·ID (scope=tenant æ—¶å¿…é¡»)")
    parser.add_argument("--priority", type=int, default=5, help="æ–‡æ¡£ä¼˜å…ˆçº§ 1-10")
    parser.add_argument("--init-db", action="store_true", help="åˆå§‹åŒ–æ•°æ®åº“è¡¨")
    args = parser.parse_args()

    if args.init_db:
        print("ğŸ”§ åˆå§‹åŒ–æ•°æ®åº“è¡¨...")
        Base.metadata.create_all(engine)
        print("âœ… æ•°æ®åº“è¡¨å·²åˆ›å»º")

    if args.dir:
        ingest_directory(
            directory=args.dir,
            scope=args.scope,
            domain_id=args.domain,
            tenant_id=args.tenant,
            priority=args.priority,
        )
    else:
        # é»˜è®¤: å…¥åº“æ‰€æœ‰ knowledge å­ç›®å½•
        kb_root = os.path.join(PROJECT_ROOT, "knowledge")

        if not os.path.isdir(kb_root):
            print(f"âš ï¸ knowledge/ ç›®å½•ä¸å­˜åœ¨, åˆ›å»ºç¤ºä¾‹ç›®å½•...")
            os.makedirs(os.path.join(kb_root, "kb_theory"), exist_ok=True)
            os.makedirs(os.path.join(kb_root, "kb_case_studies"), exist_ok=True)
            print(f"è¯·å°† Markdown æ–‡ä»¶æ”¾å…¥ {kb_root} åé‡æ–°è¿è¡Œ")
            return

        # å…ˆåˆå§‹åŒ–è¡¨
        Base.metadata.create_all(engine)

        # å…¥åº“ç†è®ºåº“
        theory_dir = os.path.join(kb_root, "kb_theory")
        if os.path.isdir(theory_dir):
            ingest_directory(theory_dir, scope="platform", domain_id="general")

        # å…¥åº“æ¡ˆä¾‹åº“
        case_dir = os.path.join(kb_root, "kb_case_studies")
        if os.path.isdir(case_dir):
            ingest_directory(case_dir, scope="platform")

        # å…¥åº“é¢†åŸŸåº“ (æŒ‰å­ç›®å½•åä½œä¸º domain_id)
        domain_dir = os.path.join(kb_root, "kb_domain")
        if os.path.isdir(domain_dir):
            for sub in sorted(os.listdir(domain_dir)):
                sub_path = os.path.join(domain_dir, sub)
                if os.path.isdir(sub_path):
                    ingest_directory(sub_path, scope="domain", domain_id=sub)


if __name__ == "__main__":
    main()
