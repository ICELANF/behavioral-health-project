#!/usr/bin/env python3
"""
BHP v3 â€” 180å¤©å…¨æµç¨‹æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨
====================================

æ¨¡æ‹Ÿè§„æ¨¡:
  - 180 å¤©è¿è¥ (2025-08-15 â†’ 2026-02-10)
  - 1,200 æ³¨å†Œç”¨æˆ· (å‰60å¤©å¯†é›†æ³¨å†Œ)
  - æ—¥å‡ 300 æ´»è·ƒäººæ¬¡
  - è¦†ç›–: æ³¨å†Œâ†’è¯Šæ–­â†’è¯„ä¼°â†’å¯¹è¯â†’æ‰“å¡â†’è¿½è¸ªâ†’ç§¯åˆ†â†’é˜¶æ®µè½¬å˜â†’ä¸“å®¶å®¡æ ¸

ç”Ÿæˆè¡¨ (20å¼ ):
  users, change_causes, user_change_cause_scores, intervention_strategies,
  health_competency_assessments, comb_assessments, self_efficacy_assessments,
  obstacle_assessments, support_assessments, intervention_outcomes,
  stage_transition_logs, point_events, user_point_balances, incentive_rewards,
  user_rewards, assessment_sessions, batch_answers, llm_call_logs,
  rag_query_logs, knowledge_chunks

ç”¨æ³•:
  # SQLite (æœ¬åœ°æµ‹è¯•)
  python scripts/generate_sim_data.py

  # PostgreSQL (ç”Ÿäº§)
  DATABASE_URL=postgresql://bhp:xxx@localhost:5432/bhp python scripts/generate_sim_data.py

  # è‡ªå®šä¹‰å‚æ•°
  python scripts/generate_sim_data.py --users 500 --days 90 --dau 150
"""

import os
import sys
import json
import random
import hashlib
import math
import uuid
from datetime import datetime, timedelta, timezone
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Optional

# â”€â”€ è·¯å¾„ â”€â”€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
os.environ.setdefault("DATABASE_URL", "postgresql://postgres:difyai123456@localhost:5432/health_platform")

from sqlalchemy import create_engine, text, inspect
from sqlalchemy.orm import sessionmaker


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é…ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SimConfig:
    start_date: datetime = datetime(2025, 8, 15, tzinfo=timezone.utc)
    total_days: int = 180
    total_users: int = 1200
    target_dau: int = 300
    seed: int = 42

    # ç”¨æˆ·ç”»åƒåˆ†å¸ƒ (5ç±»)
    ARCHETYPES = {
        "motivated":   0.20,  # é«˜åŠ¨æœº: åšæŒæ‰“å¡, å¿«é€Ÿè¿›é˜¶
        "steady":      0.30,  # ç¨³å®šå‹: æŒ‰èŠ‚å¥æ¨è¿›, å¶å°”ç¼ºå‹¤
        "struggling":  0.25,  # æŒ£æ‰å‹: æ—¶åšæ—¶åœ, é˜¶æ®µåå¤
        "explorer":    0.15,  # æ¢ç´¢å‹: è¯„ä¼°/å¯¹è¯å¤š, æ‰§è¡Œå°‘
        "dropout":     0.10,  # æµå¤±å‹: æ³¨å†Œåé€æ¸ä¸æ´»è·ƒ
    }

    # æ…¢æ€§ç—…ç§åˆ†å¸ƒ
    CONDITIONS = [
        ("metabolic_syndrome", 0.30),  # ä»£è°¢ç»¼åˆå¾
        ("type2_diabetes",     0.25),  # 2å‹ç³–å°¿ç—…
        ("hypertension",       0.20),  # é«˜è¡€å‹
        ("dyslipidemia",       0.15),  # è¡€è„‚å¼‚å¸¸
        ("obesity",            0.10),  # è‚¥èƒ–
    ]

    # å¹´é¾„åˆ†å¸ƒ (40-75 ä¸ºä¸»)
    AGE_DIST = [(35, 45, 0.15), (45, 55, 0.30), (55, 65, 0.35), (65, 78, 0.20)]

    # ä¸­å›½åŸå¸‚åˆ†å¸ƒ
    CITIES = [
        "åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·", "å—äº¬", "æˆéƒ½", "æ­¦æ±‰",
        "é‡åº†", "è¥¿å®‰", "é•¿æ²™", "éƒ‘å·", "æµå—", "æ²ˆé˜³", "é’å²›", "å¤§è¿",
        "è‹å·", "å¦é—¨", "æ˜†æ˜", "åˆè‚¥", "ç¦å·", "å—æ˜Œ", "è´µé˜³", "å…°å·",
    ]

    # å§“æ° + åå­—ç´ æ
    SURNAMES = "ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯äºè‘£è§ç¨‹æ›¹è¢é‚“è®¸"
    GIVEN_NAMES = [
        "ä¼Ÿ", "èŠ³", "ç§€è‹±", "æ•", "é™", "ä¸½", "å¼º", "ç£Š", "æ´‹", "è‰³",
        "å‹‡", "å†›", "æ°", "å¨Ÿ", "æ¶›", "æ˜", "è¶…", "ç§€å…°", "éœ", "å¹³",
        "åˆš", "æ¡‚è‹±", "å", "ç‰å…°", "è", "çº¢", "ç‰", "å‡¤", "è¾‰", "å»ºå",
        "å»ºå›½", "å»ºå†›", "æ–‡", "å¿—å¼º", "æ°¸", "æ˜¥", "é›ª", "æ…§", "å©·", "æµ©",
    ]


CFG = SimConfig()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# éšæœºå·¥å…·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RNG:
    """å¯å¤ç°çš„éšæœºæ•°ç”Ÿæˆå™¨"""
    def __init__(self, seed):
        self.r = random.Random(seed)

    def choice(self, seq):
        return self.r.choice(seq)

    def choices(self, seq, weights=None, k=1):
        return self.r.choices(seq, weights=weights, k=k)

    def randint(self, a, b):
        return self.r.randint(a, b)

    def uniform(self, a, b):
        return self.r.uniform(a, b)

    def gauss(self, mu, sigma):
        return self.r.gauss(mu, sigma)

    def shuffle(self, x):
        self.r.shuffle(x)

    def random(self):
        return self.r.random()

    def weighted_choice(self, items_weights):
        items = [i for i, _ in items_weights]
        weights = [w for _, w in items_weights]
        return self.choices(items, weights=weights, k=1)[0]

    def phone(self):
        prefixes = ["130","131","132","133","134","135","136","137","138","139",
                     "150","151","152","153","155","156","157","158","159",
                     "170","171","172","173","175","176","177","178",
                     "180","181","182","183","184","185","186","187","188","189"]
        return self.choice(prefixes) + "".join([str(self.randint(0,9)) for _ in range(8)])

    def chinese_name(self):
        s = self.choice(CFG.SURNAMES)
        g = self.choice(CFG.GIVEN_NAMES)
        return s + g


rng = RNG(CFG.seed)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç”¨æˆ·ç”»åƒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

STAGES = ["S0", "S1", "S2", "S3", "S4", "S5", "S6"]
GROWTH_LEVELS = ["G0", "G1", "G2", "G3", "G4"]
READINESS = ["L1", "L2", "L3", "L4", "L5"]
CULTIVATION = ["startup", "adaptation", "stability", "internalization"]

# å„ç”»åƒçš„è¡Œä¸ºå‚æ•°
ARCHETYPE_PARAMS = {
    "motivated": {
        "daily_active_prob": 0.85,   # æ¯å¤©æ´»è·ƒæ¦‚ç‡
        "checkin_prob":      0.90,   # æ‰“å¡æ¦‚ç‡
        "chat_prob":         0.40,   # å‘èµ·å¯¹è¯æ¦‚ç‡
        "assessment_speed":  1.5,    # è¯„ä¼°å®Œæˆé€Ÿåº¦å€ç‡
        "completion_rate":   (0.75, 0.95),  # ä»»åŠ¡å®Œæˆç‡èŒƒå›´
        "stage_advance_rate": 0.015,  # æ¯å¤©è¿›é˜¶æ¦‚ç‡
        "dropout_prob":      0.001,  # æ¯å¤©æµå¤±æ¦‚ç‡
        "init_stage_dist":   [(0,0.05),(1,0.15),(2,0.40),(3,0.30),(4,0.10)],
    },
    "steady": {
        "daily_active_prob": 0.65,
        "checkin_prob":      0.70,
        "chat_prob":         0.25,
        "assessment_speed":  1.0,
        "completion_rate":   (0.50, 0.80),
        "stage_advance_rate": 0.008,
        "dropout_prob":      0.003,
        "init_stage_dist":   [(0,0.10),(1,0.25),(2,0.35),(3,0.25),(4,0.05)],
    },
    "struggling": {
        "daily_active_prob": 0.40,
        "checkin_prob":      0.45,
        "chat_prob":         0.35,
        "assessment_speed":  0.7,
        "completion_rate":   (0.20, 0.55),
        "stage_advance_rate": 0.004,
        "dropout_prob":      0.008,
        "init_stage_dist":   [(0,0.25),(1,0.35),(2,0.25),(3,0.10),(4,0.05)],
    },
    "explorer": {
        "daily_active_prob": 0.55,
        "checkin_prob":      0.30,
        "chat_prob":         0.65,
        "assessment_speed":  1.2,
        "completion_rate":   (0.30, 0.60),
        "stage_advance_rate": 0.005,
        "dropout_prob":      0.005,
        "init_stage_dist":   [(0,0.15),(1,0.30),(2,0.30),(3,0.20),(4,0.05)],
    },
    "dropout": {
        "daily_active_prob": 0.25,
        "checkin_prob":      0.20,
        "chat_prob":         0.15,
        "assessment_speed":  0.5,
        "completion_rate":   (0.10, 0.35),
        "stage_advance_rate": 0.001,
        "dropout_prob":      0.025,
        "init_stage_dist":   [(0,0.40),(1,0.30),(2,0.20),(3,0.08),(4,0.02)],
    },
}


@dataclass
class SimUser:
    """æ¨¡æ‹Ÿç”¨æˆ·çŠ¶æ€æœº"""
    id: int
    phone: str
    nickname: str
    password_hash: str
    role: str
    archetype: str
    condition: str
    city: str
    age: int
    gender: str
    register_day: int  # æ³¨å†Œåœ¨ç¬¬å‡ å¤©

    # åŠ¨æ€çŠ¶æ€
    current_stage: int = 0        # S0-S6 çš„æ•°å­—
    growth_level: int = 0         # G0-G4
    readiness: int = 1            # L1-L5
    cultivation: int = 0          # åŸ¹å…»é˜¶æ®µ index
    spi_score: float = 30.0       # æˆåŠŸå¯èƒ½æ€§æŒ‡æ•°
    health_competency: str = "Lv0"
    is_active: bool = True
    dropped_out: bool = False

    # ç§¯åˆ†
    growth_points: int = 0
    contribution_points: int = 0
    influence_points: int = 0
    streak_days: int = 0
    longest_streak: int = 0
    tasks_completed_total: int = 0
    assessments_completed: int = 0
    last_checkin_day: int = -999

    # è¯„ä¼°è¿›åº¦
    completed_batches: list = field(default_factory=list)
    assessment_session_id: Optional[int] = None
    diagnostic_done: bool = False
    full_diagnostic_done: bool = False

    @property
    def params(self):
        return ARCHETYPE_PARAMS[self.archetype]

    @property
    def stage_str(self):
        return STAGES[min(self.current_stage, 6)]

    @property
    def growth_str(self):
        return GROWTH_LEVELS[min(self.growth_level, 4)]

    def daily_active_prob(self, day_offset):
        """æ´»è·ƒæ¦‚ç‡éšæ—¶é—´è¡°å‡ (dropout ç±»å‹è¡°å‡å¿«)"""
        base = self.params["daily_active_prob"]
        days_since_reg = day_offset - self.register_day
        if self.dropped_out:
            return 0.02  # å¶å°”å›æ¥çœ‹çœ‹
        if self.archetype == "dropout":
            decay = math.exp(-0.02 * days_since_reg)
        elif self.archetype == "struggling":
            decay = math.exp(-0.005 * days_since_reg)
        else:
            decay = max(0.8, math.exp(-0.001 * days_since_reg))
        return base * decay


def generate_users() -> list[SimUser]:
    """ç”Ÿæˆ 1200 ä¸ªæ¨¡æ‹Ÿç”¨æˆ·, æ³¨å†Œåˆ†å¸ƒåœ¨ 180 å¤©å†…"""
    users = []

    # æ³¨å†Œåˆ†å¸ƒ: å‰ 30 å¤©å¯†é›† (60%), 30-90 å¤©ä¸­ç­‰ (25%), 90-180 å¤©ç¨€ç– (15%)
    reg_days = []
    n = CFG.total_users
    for _ in range(int(n * 0.60)):
        reg_days.append(rng.randint(0, 29))
    for _ in range(int(n * 0.25)):
        reg_days.append(rng.randint(30, 89))
    for _ in range(n - len(reg_days)):
        reg_days.append(rng.randint(90, 179))
    reg_days.sort()

    used_phones = set()
    archetypes = list(CFG.ARCHETYPES.keys())
    archetype_weights = list(CFG.ARCHETYPES.values())

    # Start IDs at 1001 to avoid conflict with seed users (IDs 2-33)
    SIM_ID_OFFSET = 1000
    for i, reg_day in enumerate(reg_days):
        uid = i + 1 + SIM_ID_OFFSET

        # æ‰‹æœºå·å»é‡
        phone = rng.phone()
        while phone in used_phones:
            phone = rng.phone()
        used_phones.add(phone)

        archetype = rng.weighted_choice(list(CFG.ARCHETYPES.items()))
        condition = rng.weighted_choice(CFG.CONDITIONS)

        # å¹´é¾„
        age_bracket = rng.weighted_choice([(b, w) for b in CFG.AGE_DIST for _, _, w in [b]])
        # Fix: properly unpack
        for lo, hi, w in CFG.AGE_DIST:
            if rng.random() < w / sum(ww for _, _, ww in CFG.AGE_DIST):
                age = rng.randint(lo, hi)
                break
        else:
            age = rng.randint(45, 65)

        gender = rng.choice(["male", "female"])
        city = rng.choice(CFG.CITIES)

        # åˆå§‹é˜¶æ®µ
        init_stages = ARCHETYPE_PARAMS[archetype]["init_stage_dist"]
        init_stage = rng.weighted_choice(init_stages)

        # åˆå§‹ SPI (ä¸é˜¶æ®µç›¸å…³)
        base_spi = 15 + init_stage * 12 + rng.gauss(0, 5)
        spi = max(5, min(95, base_spi))

        # å¯†ç æ•£åˆ— (å›ºå®šæ ¼å¼, ä¸çœŸæ­£ç”¨ bcrypt åŠ é€Ÿç”Ÿæˆ)
        pw_hash = "$2b$12$" + hashlib.sha256(f"sim_{uid}_{phone}".encode()).hexdigest()[:53]

        # è§’è‰² (ä½¿ç”¨ PostgreSQL userrole æšä¸¾å€¼)
        if uid <= 5:
            role = "ADMIN"
        elif uid <= 15:
            role = "MASTER"
        elif uid <= 35:
            role = "COACH"
        else:
            role = "OBSERVER"

        user = SimUser(
            id=uid, phone=phone, nickname=rng.chinese_name(),
            password_hash=pw_hash, role=role, archetype=archetype,
            condition=condition, city=city, age=age, gender=gender,
            register_day=reg_day, current_stage=init_stage,
            spi_score=round(spi, 1),
            readiness=min(init_stage + 1, 5),
        )
        users.append(user)

    return users


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç§å­æ•°æ®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CHANGE_CAUSES = [
    ("C01", "psychological", "å¥åº·ä¿¡å¿µ", "health_belief", 1.2),
    ("C02", "psychological", "è‡ªæˆ‘æ•ˆèƒ½", "self_efficacy", 1.3),
    ("C03", "psychological", "é£é™©æ„ŸçŸ¥", "risk_perception", 1.1),
    ("C04", "psychological", "ç»“æœæœŸæœ›", "outcome_expectation", 1.0),
    ("C05", "behavioral", "è¡Œä¸ºä¹ æƒ¯", "habit_pattern", 1.2),
    ("C06", "behavioral", "è‡ªæˆ‘ç›‘æ§", "self_monitoring", 1.1),
    ("C07", "behavioral", "ç›®æ ‡è®¾å®š", "goal_setting", 1.0),
    ("C08", "behavioral", "åº”å¯¹ç­–ç•¥", "coping_strategy", 0.9),
    ("C09", "social", "ç¤¾ä¼šæ”¯æŒ", "social_support", 1.0),
    ("C10", "social", "åŒä¼´å½±å“", "peer_influence", 0.8),
    ("C11", "social", "å®¶åº­ç¯å¢ƒ", "family_environment", 1.1),
    ("C12", "environmental", "åŒ»ç–—å¯åŠæ€§", "healthcare_access", 0.9),
    ("C13", "environmental", "å¥åº·ç´ å…»", "health_literacy", 1.2),
    ("C14", "environmental", "ç»æµå› ç´ ", "economic_factor", 0.7),
    ("C15", "motivational", "å†…åœ¨åŠ¨æœº", "intrinsic_motivation", 1.4),
    ("C16", "motivational", "æ”¹å˜æ„æ„¿", "readiness_to_change", 1.3),
    ("C17", "motivational", "è¿«åˆ‡æ„Ÿ", "urgency_sense", 1.1),
    ("C18", "cognitive", "è®¤çŸ¥åå·®", "cognitive_bias", 0.9),
    ("C19", "cognitive", "çŸ¥è¯†æ°´å¹³", "knowledge_level", 1.0),
    ("C20", "cognitive", "å†³ç­–èƒ½åŠ›", "decision_making", 0.8),
]

INTERVENTION_STRATEGIES = [
    # (stage, readiness, cause_code, strategy_type, coach_script_snippet)
    ("S0", "L1", "C01", "awareness_raising", "æ‚¨æ˜¯å¦äº†è§£æ‚¨ç›®å‰çš„å¥åº·çŠ¶å†µï¼Ÿ"),
    ("S0", "L1", "C03", "risk_education",    "è®©æˆ‘ä»¬äº†è§£ä¸€ä¸‹ä¸ç®¡ç†è¡€ç³–å¯èƒ½å¸¦æ¥çš„é£é™©"),
    ("S1", "L2", "C02", "efficacy_building",  "ç›¸ä¿¡è‡ªå·±å¯ä»¥åšåˆ°ï¼Œæˆ‘ä»¬ä»å°ç›®æ ‡å¼€å§‹"),
    ("S1", "L2", "C16", "motivational_interview", "æ˜¯ä»€ä¹ˆè®©æ‚¨å¼€å§‹è€ƒè™‘æ”¹å˜ï¼Ÿ"),
    ("S2", "L3", "C05", "habit_formation",    "æ¯å¤©å›ºå®šæ—¶é—´åšä¸€ä»¶å¥åº·å°äº‹"),
    ("S2", "L3", "C07", "smart_goal",         "è®©æˆ‘ä»¬åˆ¶å®šä¸€ä¸ªå…·ä½“å¯è¡¡é‡çš„ç›®æ ‡"),
    ("S3", "L3", "C06", "self_monitoring",    "åšæŒè®°å½•æ‚¨çš„é¥®é£Ÿå’Œè¿åŠ¨"),
    ("S3", "L4", "C08", "relapse_prevention", "é‡åˆ°å›°éš¾æ—¶å¯ä»¥è¯•è¯•è¿™äº›åº”å¯¹æ–¹æ³•"),
    ("S4", "L4", "C09", "social_mobilization","å‘åŠ¨å®¶äººä¸€èµ·å‚ä¸å¥åº·ç®¡ç†"),
    ("S4", "L5", "C15", "intrinsic_reward",   "æ„Ÿå—åˆ°èº«ä½“çš„å˜åŒ–äº†å—ï¼Ÿè¿™å°±æ˜¯æœ€å¥½çš„å¥–åŠ±"),
    ("S5", "L5", "C05", "maintenance_plan",   "å»ºç«‹é•¿æœŸç»´æŒçš„æ—¥å¸¸èŠ‚å¾‹"),
    ("S5", "L5", "C11", "family_engagement",  "è®©å¥åº·æˆä¸ºå…¨å®¶äººçš„ç”Ÿæ´»æ–¹å¼"),
]

REWARDS_CATALOG = [
    ("badge",   "é¦–æ¬¡æ‰“å¡",       "å®Œæˆç¬¬ä¸€æ¬¡æ¯æ—¥æ‰“å¡",         "ğŸ¯", "growth",       10),
    ("badge",   "è¿ç»­ä¸ƒå¤©",       "è¿ç»­æ‰“å¡7å¤©",               "ğŸ”¥", "streak",       7),
    ("badge",   "è¿ç»­ä¸‰åå¤©",     "è¿ç»­æ‰“å¡30å¤©",              "ğŸ’ª", "streak",       30),
    ("badge",   "è¯„ä¼°è¾¾äºº",       "å®Œæˆæ‰€æœ‰è¯„ä¼°æ‰¹æ¬¡",           "ğŸ“‹", "growth",       500),
    ("badge",   "çŸ¥è¯†æ¢ç´¢è€…",     "ç´¯è®¡å‘èµ·20æ¬¡çŸ¥è¯†é—®ç­”",       "ğŸ“š", "growth",       200),
    ("title",   "å¥åº·æ–°æ‰‹",       "è¾¾åˆ°G1æˆé•¿ç­‰çº§",            "ğŸŒ±", "growth",       100),
    ("title",   "å¥åº·è·µè¡Œè€…",     "è¾¾åˆ°G2æˆé•¿ç­‰çº§",            "ğŸŒ¿", "growth",       300),
    ("title",   "å¥åº·è¾¾äºº",       "è¾¾åˆ°G3æˆé•¿ç­‰çº§",            "ğŸŒ³", "growth",       800),
    ("title",   "å¥åº·å¯¼å¸ˆ",       "è¾¾åˆ°G4æˆé•¿ç­‰çº§",            "ğŸ‘‘", "growth",       2000),
    ("feature", "Coach æ·±åº¦å¯¹è¯",  "è§£é”ä¸AIæ•™ç»ƒçš„æ·±åº¦åˆ†æåŠŸèƒ½", "ğŸ§ ", "growth",       150),
    ("feature", "ä¸ªæ€§åŒ–æŠ¥å‘Š",     "è§£é”æ¯å‘¨ä¸ªæ€§åŒ–å¥åº·æŠ¥å‘Š",     "ğŸ“Š", "growth",       250),
    ("item",    "å¥åº·æ‰‹ç¯ä¼˜æƒ åˆ¸", "åˆä½œå“ç‰Œå¥åº·è®¾å¤‡æŠ˜æ‰£",       "âŒš", "contribution", 500),
]

# Coach å¯¹è¯æ¨¡æ¿ (æŒ‰æ„å›¾åˆ†ç±»)
CHAT_TEMPLATES = {
    "knowledge_query": [
        "ä»£è°¢ç»¼åˆå¾çš„é¥®é£ŸåŸåˆ™æ˜¯ä»€ä¹ˆï¼Ÿ",
        "äºŒç”²åŒèƒå’Œè¿åŠ¨å¯ä»¥ä¸€èµ·å—ï¼Ÿ",
        "è¡€ç³–å¤šå°‘ç®—æ­£å¸¸èŒƒå›´ï¼Ÿ",
        "é«˜è¡€å‹æ‚£è€…èƒ½ä¸èƒ½å–å’–å•¡ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯ç³–åŒ–è¡€çº¢è›‹ç™½ï¼Ÿ",
        "å‡é‡å¯¹è¡€è„‚æœ‰ä»€ä¹ˆå¸®åŠ©ï¼Ÿ",
        "å¦‚ä½•åˆ¤æ–­è‡ªå·±æ˜¯å¦æœ‰èƒ°å²›ç´ æŠµæŠ—ï¼Ÿ",
        "ä½GIé¥®é£Ÿå…·ä½“æ€ä¹ˆåšï¼Ÿ",
        "è¿åŠ¨é™å‹çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä¸­åŒ»å¦‚ä½•çœ‹å¾…ä»£è°¢ç»¼åˆå¾ï¼Ÿ",
        "ç—°æ¹¿ä½“è´¨æ€ä¹ˆè°ƒç†ï¼Ÿ",
        "ç³–å°¿ç—…å‰æœŸèƒ½é€†è½¬å—ï¼Ÿ",
        "è‚åŠŸèƒ½ä¸å¥½èƒ½åƒé™è„‚è¯å—ï¼Ÿ",
        "é—´æ­‡æ€§æ–­é£Ÿé€‚åˆç³–å°¿ç—…äººå—ï¼Ÿ",
        "ä»€ä¹ˆè¿åŠ¨å¯¹é™è¡€ç³–æœ€æœ‰æ•ˆï¼Ÿ",
    ],
    "emotional_support": [
        "æˆ‘ä»Šå¤©åˆæ²¡ç®¡ä½å˜´ï¼Œå¥½è‡ªè´£",
        "åšæŒäº†ä¸€å‘¨ï¼Œçªç„¶å¾ˆæƒ³æ”¾å¼ƒ",
        "å®¶äººä¸ç†è§£æˆ‘ä¸ºä»€ä¹ˆè¦å¿Œå£",
        "è¡€ç³–ä¸€ç›´é™ä¸ä¸‹æ¥å¥½ç„¦è™‘",
        "åŒäº‹èšé¤ä¸å¥½æ„æ€æ‹’ç»",
        "æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œå®Œå…¨ä¸æƒ³åŠ¨",
        "åŒ»ç”Ÿè¯´å¯èƒ½è¦æ‰“èƒ°å²›ç´ ï¼Œå¾ˆå®³æ€•",
        "å‡äº†5æ–¤åˆåå¼¹äº†ï¼Œå¾ˆæ²®ä¸§",
        "è€ä¼´èµ°äº†ä»¥åä¸€ä¸ªäººä¹Ÿä¸æƒ³åšé¥­äº†",
        "å¹´çºªå¤§äº†è®°æ€§ä¸å¥½æ€»å¿˜åƒè¯",
    ],
    "prescription_adjust": [
        "ä¸Šå‘¨çš„æ–¹æ¡ˆæ„Ÿè§‰å¤ªéš¾äº†ï¼Œèƒ½è°ƒæ•´å—ï¼Ÿ",
        "æˆ‘çš„è¿åŠ¨é‡æƒ³åŠ ä¸€äº›",
        "æœ€è¿‘å·¥ä½œå¿™æƒ³å‡å°‘æ‰“å¡é¢‘ç‡",
        "æƒ³æŠŠèµ°è·¯æ¢æˆæ¸¸æ³³å¯ä»¥å—ï¼Ÿ",
        "é¥®é£Ÿæ–¹æ¡ˆé‡Œé¢çš„ç²—ç²®æˆ‘åƒä¸æƒ¯",
        "æˆ‘ç°åœ¨çŠ¶æ€å¥½äº†å¾ˆå¤šï¼Œæƒ³æŒ‘æˆ˜æ›´é«˜ç›®æ ‡",
    ],
    "casual": [
        "ä½ å¥½",
        "ä»Šå¤©å¤©æ°”ä¸é”™",
        "è°¢è°¢ä½ çš„å»ºè®®",
        "æˆ‘çŸ¥é“äº†",
        "å¥½çš„",
        "æ™šå®‰",
    ],
}

COACH_RESPONSES = {
    "knowledge_query": [
        "æ ¹æ®æœ€æ–°ä¸´åºŠæŒ‡å—ï¼Œå»ºè®®æ‚¨â€¦",
        "è¿™ä¸ªé—®é¢˜å¾ˆå¥½ã€‚ä»è¥å…»å­¦è§’åº¦æ¥çœ‹â€¦",
        "æ ¹æ®æ‚¨çš„ä¸ªäººæƒ…å†µï¼Œæˆ‘å»ºè®®â€¦",
    ],
    "emotional_support": [
        "ç†è§£æ‚¨çš„æ„Ÿå—ï¼Œæ”¹å˜ä»æ¥ä¸æ˜¯ä¸€å¸†é£é¡ºçš„â€¦",
        "æ‚¨å·²ç»å¾ˆæ£’äº†ï¼ŒåšæŒåˆ°ç°åœ¨æœ¬èº«å°±æ˜¯è¿›æ­¥â€¦",
        "æˆ‘ä»¬å¯ä»¥ä¸€èµ·æƒ³æƒ³é€‚åˆæ‚¨çš„åº”å¯¹æ–¹æ³•â€¦",
    ],
    "prescription_adjust": [
        "å¥½çš„ï¼Œæ ¹æ®æ‚¨çš„åé¦ˆæˆ‘æ¥è°ƒæ•´ä¸€ä¸‹â€¦",
        "äº†è§£äº†ï¼Œæˆ‘ä»¬æŠŠç›®æ ‡é™ä½ä¸€ç‚¹â€¦",
        "å¾ˆé«˜å…´æ‚¨çŠ¶æ€å¥½äº†ï¼Œæˆ‘æ¥åˆ¶å®šæ–°æ–¹æ¡ˆâ€¦",
    ],
    "casual": [
        "æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ",
        "ä¸å®¢æ°”ï¼Œéšæ—¶å¯ä»¥æ‰¾æˆ‘èŠå¤©ã€‚",
        "ç¥æ‚¨ä»Šå¤©æ„‰å¿«ï¼",
    ],
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç”Ÿæˆå¼•æ“
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SimDataGenerator:
    def __init__(self, config: SimConfig):
        self.cfg = config
        self.users: list[SimUser] = []
        self.records = defaultdict(list)  # table_name â†’ [dict, ...]
        self.counters = defaultdict(int)  # auto-increment counters

    def next_id(self, table: str) -> int:
        self.counters[table] += 1
        return self.counters[table]

    def ts(self, day: int, hour: int = 8, minute: int = 0) -> datetime:
        """æ—¥æœŸè½¬ datetime"""
        return self.cfg.start_date + timedelta(days=day, hours=hour, minutes=minute)

    def ts_rand(self, day: int, hour_range=(7, 22)) -> datetime:
        """éšæœºæ—¶é—´ç‚¹"""
        h = rng.randint(hour_range[0], hour_range[1])
        m = rng.randint(0, 59)
        s = rng.randint(0, 59)
        return self.cfg.start_date + timedelta(days=day, hours=h, minutes=m, seconds=s)

    # â”€â”€ 1. ç§å­æ•°æ® â”€â”€

    def gen_seed_data(self):
        """å­—å…¸è¡¨ + ç­–ç•¥è¡¨ + å¥–åŠ±è¡¨"""
        print("  [ç§å­] change_causes Ã— 20")
        for code, cat, name_zh, name_en, weight in CHANGE_CAUSES:
            self.records["change_causes"].append({
                "id": code, "category": cat, "name_zh": name_zh,
                "name_en": name_en, "weight": weight,
                "description": f"{name_zh}({name_en})å¯¹è¡Œä¸ºæ”¹å˜çš„å½±å“è¯„ä¼°",
                "assessment_question": f"è¯·è¯„ä¼°æ‚¨åœ¨ã€Œ{name_zh}ã€æ–¹é¢çš„çŠ¶æ€(1-5åˆ†)",
            })

        print("  [ç§å­] intervention_strategies Ã— 12")
        cause_cat_map = {c[0]: c[1] for c in CHANGE_CAUSES}
        cause_name_map = {c[0]: c[2] for c in CHANGE_CAUSES}
        stage_name_map = {"S0":"å‰æ„å‘","S1":"æ„å‘","S2":"å‡†å¤‡","S3":"è¡ŒåŠ¨","S4":"ç»´æŒ","S5":"å·©å›º"}
        for stage, readiness, cause, stype, script in INTERVENTION_STRATEGIES:
            self.records["intervention_strategies"].append({
                "id": self.next_id("intervention_strategies"),
                "stage_code": stage, "readiness_level": readiness,
                "stage_name": stage_name_map.get(stage, stage),
                "cause_code": cause,
                "cause_category": cause_cat_map.get(cause, ""),
                "cause_name": cause_name_map.get(cause, ""),
                "strategy_type": stype, "coach_script": script,
            })

        print("  [ç§å­] incentive_rewards Ã— 12")
        for rtype, name, desc, icon, dim, thresh in REWARDS_CATALOG:
            self.records["incentive_rewards"].append({
                "id": self.next_id("incentive_rewards"),
                "reward_type": rtype, "name": name, "description": desc,
                "icon": icon, "unlock_dimension": dim, "unlock_threshold": thresh,
            })

        print("  [ç§å­] knowledge_chunks Ã— 50")
        doc_types = ["clinical_guideline", "theory_framework", "intervention_strategy",
                     "behavior_guide", "tcm_principle", "nutrition_guide"]
        topics = [
            "ä»£è°¢ç»¼åˆå¾æ¦‚è¿°", "2å‹ç³–å°¿ç—…ç®¡ç†", "é«˜è¡€å‹é¥®é£ŸæŒ‡å¯¼", "è¡€è„‚å¼‚å¸¸è¿åŠ¨æ–¹æ¡ˆ",
            "TTMè¡Œä¸ºæ”¹å˜æ¨¡å‹", "åŠ¨æœºè®¿è°ˆæŠ€æœ¯", "COM-Bè¡Œä¸ºæ¨¡å‹", "è‡ªæˆ‘æ•ˆèƒ½ç†è®º",
            "è®¤çŸ¥è¡Œä¸ºç–—æ³•åŸºç¡€", "æ­£å¿µå‡å‹æŠ€æœ¯", "ä¹ æƒ¯å…»æˆç§‘å­¦", "ç¤¾ä¼šæ”¯æŒç†è®º",
            "ç³–å°¿ç—…è¶³é¢„é˜²", "èƒ°å²›ç´ æŠµæŠ—æœºåˆ¶", "GLP-1å—ä½“æ¿€åŠ¨å‰‚", "SGLT2æŠ‘åˆ¶å‰‚",
            "å…«æ®µé”¦åŠŸæ³•", "å¤ªææ‹³å¥èº«", "ä¸­åŒ»ä½“è´¨è¾¨è¯†", "ç—°æ¹¿ä½“è´¨è°ƒç†",
            "ä½GIé¥®é£ŸåŸåˆ™", "åœ°ä¸­æµ·é¥®é£Ÿ", "DASHé¥®é£Ÿ", "é—´æ­‡æ€§æ–­é£Ÿ",
            "æœ‰æ°§è¿åŠ¨å¤„æ–¹", "æŠ—é˜»è®­ç»ƒæŒ‡å¯¼", "æŸ”éŸ§æ€§è®­ç»ƒ", "å¹³è¡¡è®­ç»ƒ",
            "ç¡çœ å«ç”ŸæŒ‡å¯¼", "å‹åŠ›ç®¡ç†æŠ€æœ¯", "ç¤¾äº¤å¤„æ–¹", "å®¶åº­å¥åº·ç®¡ç†",
            "è¡€ç³–ç›‘æµ‹è§„èŒƒ", "è¡€å‹æµ‹é‡æŒ‡å—", "BMIä¸è…°å›´æ ‡å‡†", "å®éªŒå®¤æ£€æŸ¥è§£è¯»",
            "S0é˜¶æ®µå¹²é¢„ç­–ç•¥", "S1é˜¶æ®µå¹²é¢„ç­–ç•¥", "S2é˜¶æ®µå¹²é¢„ç­–ç•¥", "S3é˜¶æ®µå¹²é¢„ç­–ç•¥",
            "è¡Œä¸ºæ¨¡å¼åˆ†å‹Aå‹", "è¡Œä¸ºæ¨¡å¼åˆ†å‹Bå‹", "è¡Œä¸ºæ¨¡å¼åˆ†å‹Cå‹",
            "å¤§äº”äººæ ¼ä¸å¥åº·", "å¤–å‘æ€§å¥åº·æ•ˆåº”", "å°½è´£æ€§å¥åº·æ•ˆåº”",
            "180å¤©ä»£è°¢é‡å»ºæ–¹æ¡ˆ", "90å¤©è¡€ç³–è¾¾æ ‡è·¯å¾„", "60å¤©è¡€å‹ç®¡ç†", "30å¤©é¥®é£Ÿé©å‘½",
        ]
        for i, topic in enumerate(topics):
            self.records["knowledge_chunks"].append({
                "id": self.next_id("knowledge_chunks"),
                "chunk_id": f"kc_{i+1:04d}",
                "source": f"knowledge/{doc_types[i % len(doc_types)]}/{topic}.md",
                "doc_type": doc_types[i % len(doc_types)],
                "section": topic,
                "seq": i % 5,
                "char_count": rng.randint(300, 1500),
                "text_preview": f"[{topic}] æœ¬ç« èŠ‚è¯¦ç»†ä»‹ç»äº†{topic}çš„æ ¸å¿ƒå†…å®¹...",
                "created_at": self.ts(0).isoformat(),
                "updated_at": self.ts(0).isoformat(),
            })

    # â”€â”€ 2. ç”¨æˆ· â”€â”€

    def gen_users(self):
        self.users = generate_users()
        print(f"  [ç”¨æˆ·] users Ã— {len(self.users)}")

        for u in self.users:
            reg_ts = self.ts_rand(u.register_day, (8, 20))
            username = f"sim_{u.id:04d}"
            self.records["users"].append({
                "id": u.id, "phone": u.phone,
                "username": username,
                "email": f"{username}@sim.bhp.local",
                "nickname": u.nickname,
                "password_hash": u.password_hash, "role": u.role,
                "is_active": True, "health_competency_level": "Lv0",
                "current_stage": u.stage_str, "growth_level": u.growth_str,
                "created_at": reg_ts.isoformat(),
                "updated_at": reg_ts.isoformat(),
                "last_login_at": reg_ts.isoformat(),
                "avatar_url": "",
            })

    # â”€â”€ 3. æ¯æ—¥æ¨¡æ‹Ÿ â”€â”€

    def simulate_day(self, day: int):
        """æ¨¡æ‹Ÿç¬¬ day å¤© (0-indexed) çš„æ‰€æœ‰æ´»åŠ¨"""
        active_today = []

        for user in self.users:
            if user.register_day > day:
                continue  # è¿˜æ²¡æ³¨å†Œ
            if not user.is_active and not user.dropped_out:
                continue

            prob = user.daily_active_prob(day)
            if rng.random() < prob:
                active_today.append(user)

        # â”€â”€ æ–°ç”¨æˆ·é¦–æ—¥: è¯Šæ–­ + è¯„ä¼°å¯åŠ¨ â”€â”€
        for user in active_today:
            if user.register_day == day:
                self._do_onboarding(user, day)

        # â”€â”€ æ—¥å¸¸æ´»åŠ¨ â”€â”€
        for user in active_today:
            ts = self.ts_rand(day)

            # æ‰“å¡
            if rng.random() < user.params["checkin_prob"]:
                self._do_checkin(user, day, ts)

            # ä»»åŠ¡å®Œæˆ (æ—¥å¸¸å¹²é¢„)
            if rng.random() < user.params["completion_rate"][1]:
                self._do_daily_outcome(user, day, ts)

            # å¯¹è¯
            if rng.random() < user.params["chat_prob"]:
                n_chats = rng.randint(1, 3)
                for _ in range(n_chats):
                    self._do_chat(user, day)

            # è¯„ä¼°æ¨è¿›
            days_since_reg = day - user.register_day
            assessment_interval = int(7 / user.params["assessment_speed"])
            if (days_since_reg > 0 and
                days_since_reg % max(assessment_interval, 3) == 0 and
                len(user.completed_batches) < 12):
                self._do_assessment_batch(user, day)

            # é˜¶æ®µè¿›é˜¶æ£€æŸ¥
            if rng.random() < user.params["stage_advance_rate"]:
                self._do_stage_advance(user, day)

            # æµå¤±æ£€æŸ¥
            if rng.random() < user.params["dropout_prob"]:
                user.dropped_out = True

        # æ›´æ–° last_login
        for user in active_today:
            for rec in self.records["users"]:
                if rec["id"] == user.id:
                    rec["last_login_at"] = self.ts_rand(day).isoformat()
                    rec["current_stage"] = user.stage_str
                    rec["growth_level"] = user.growth_str
                    break

        return len(active_today)

    # â”€â”€ é¦–æ—¥å¼•å¯¼ â”€â”€

    def _do_onboarding(self, user: SimUser, day: int):
        ts = self.ts_rand(day, (8, 12))

        # æœ€å°å¯åŠ¨è¯Šæ–­
        user.diagnostic_done = True

        # åˆ›å»ºè¯„ä¼° session
        sid = self.next_id("assessment_sessions")
        user.assessment_session_id = sid
        self.records["assessment_sessions"].append({
            "id": sid, "user_id": user.id, "status": "in_progress",
            "completed_batches": json.dumps([]),
            "pending_batches": json.dumps(["B1_TTM7_CORE", "B2_SPI_QUICK"]),
            "total_questions_answered": 0, "total_questions": 176,
            "partial_results": json.dumps({}),
            "started_at": ts.isoformat(), "last_activity": ts.isoformat(),
            "completed_at": None,
            "expires_at": (ts + timedelta(days=30)).isoformat(),
        })

        # å®Œæˆ B1 + B2 (é¦–æ—¥å¿…åš)
        self._submit_batch(user, "B1_TTM7_CORE", 7, day, ts)
        self._submit_batch(user, "B2_SPI_QUICK", 5, day,
                          ts + timedelta(minutes=rng.randint(3, 10)))

        # åŠ¨å› è¯„åˆ† (åŸºäºåˆå§‹è¯Šæ–­)
        for cause_code, _, name_zh, _, weight in CHANGE_CAUSES:
            base = 2 + user.current_stage * 0.3
            score = max(1, min(5, int(base + rng.gauss(0, 1))))
            self.records["user_change_cause_scores"].append({
                "id": self.next_id("user_change_cause_scores"),
                "user_id": user.id, "assessment_id": sid,
                "cause_id": cause_code,
                "score": score,
                "created_at": ts.isoformat(),
            })

        # å¥åº·ç´ å…»è¯„ä¼°
        base_score = 30 + user.current_stage * 8 + rng.gauss(0, 10)
        hc_score = max(10, min(100, base_score))
        hc_level = "Lv0" if hc_score < 40 else "Lv1" if hc_score < 60 else "Lv2" if hc_score < 80 else "Lv3"
        user.health_competency = hc_level
        self.records["health_competency_assessments"].append({
            "id": self.next_id("health_competency_assessments"),
            "user_id": user.id,
            "answers": json.dumps({"total_score": round(hc_score, 1)}),
            "level_scores": json.dumps({
                "knowledge": round(hc_score * rng.uniform(0.8, 1.2), 1),
                "attitude": round(hc_score * rng.uniform(0.9, 1.1), 1),
                "practice": round(hc_score * rng.uniform(0.7, 1.3), 1),
            }),
            "current_level": hc_level,
            "recommended_content_stage": CULTIVATION[min(user.cultivation, 3)],
            "created_at": ts.isoformat(),
        })

        # COMB è¯„ä¼°
        cap = round(rng.uniform(30, 80), 1)
        opp = round(rng.uniform(40, 85), 1)
        mot = round(rng.uniform(20 + user.current_stage * 10, 90), 1)
        bottleneck = min([("capability", cap), ("opportunity", opp), ("motivation", mot)],
                        key=lambda x: x[1])[0]
        self.records["comb_assessments"].append({
            "id": self.next_id("comb_assessments"),
            "user_id": user.id,
            "answers": json.dumps({"capability": cap, "opportunity": opp, "motivation": mot}),
            "dimension_scores": json.dumps({"capability": cap, "opportunity": opp, "motivation": mot}),
            "bottleneck": bottleneck,
            "total_score": round((cap + opp + mot) / 3, 1),
            "created_at": ts.isoformat(),
        })

        # è‡ªæˆ‘æ•ˆèƒ½è¯„ä¼°
        se_score = round(20 + user.current_stage * 12 + rng.gauss(0, 8), 1)
        se_score = max(10, min(100, se_score))
        se_level = "low" if se_score < 40 else "medium" if se_score < 70 else "high"
        self.records["self_efficacy_assessments"].append({
            "id": self.next_id("self_efficacy_assessments"),
            "user_id": user.id,
            "answers": json.dumps({"raw_score": se_score}),
            "avg_score": se_score,
            "level": se_level, "created_at": ts.isoformat(),
        })

        # éšœç¢è¯„ä¼°
        obstacles = rng.choices(
            ["time", "motivation", "knowledge", "social", "cost", "physical", "emotional"],
            k=rng.randint(2, 4))
        obs_score = round(rng.uniform(20, 70), 1)
        self.records["obstacle_assessments"].append({
            "id": self.next_id("obstacle_assessments"),
            "user_id": user.id,
            "answers": json.dumps({"total_score": obs_score}),
            "category_scores": json.dumps({o: round(rng.uniform(1, 5), 1) for o in obstacles}),
            "top_obstacles": json.dumps(obstacles),
            "rx_adjustments": json.dumps({"reduce_intensity": obstacles[0] == "physical"}),
            "created_at": ts.isoformat(),
        })

        # æ”¯æŒè¯„ä¼°
        support_score = round(rng.uniform(30, 80), 1)
        weakest = rng.choice(["family", "peer", "professional", "community"])
        self.records["support_assessments"].append({
            "id": self.next_id("support_assessments"),
            "user_id": user.id,
            "answers": json.dumps({"raw_score": support_score}),
            "layer_scores": json.dumps({
                "family": round(rng.uniform(20, 90), 1),
                "peer": round(rng.uniform(10, 70), 1),
                "professional": round(rng.uniform(30, 80), 1),
                "community": round(rng.uniform(15, 60), 1),
            }),
            "total_score": support_score,
            "support_level": "low" if support_score < 40 else "medium" if support_score < 65 else "high",
            "weakest_layer": weakest,
            "created_at": ts.isoformat(),
        })

        # é¦–æ¬¡å¯¹è¯ (æ¬¢è¿)
        self._do_chat(user, day, intent="casual",
                     msg="ä½ å¥½ï¼Œæˆ‘åˆšæ³¨å†Œ", resp="æ¬¢è¿åŠ å…¥BHPå¥åº·ç®¡ç†å¹³å°ï¼æˆ‘æ˜¯æ‚¨çš„AIå¥åº·æ•™ç»ƒâ€¦")

        # é¦–æ—¥ç§¯åˆ†
        self._add_points(user, "registration", "growth", 20, "å®Œæˆæ³¨å†Œ", day)
        self._add_points(user, "first_assessment", "growth", 30, "é¦–æ¬¡è¯„ä¼°å®Œæˆ", day)

    # â”€â”€ è¯„ä¼°æ‰¹æ¬¡æäº¤ â”€â”€

    def _submit_batch(self, user: SimUser, batch_id: str, q_count: int,
                     day: int, ts: datetime):
        if batch_id in user.completed_batches:
            return  # å¹‚ç­‰

        answers = {}
        for q in range(1, q_count + 1):
            answers[f"q{q}"] = rng.randint(1, 5)

        self.records["batch_answers"].append({
            "id": self.next_id("batch_answers"),
            "session_id": user.assessment_session_id or 1,
            "user_id": user.id,
            "batch_id": batch_id,
            "questionnaire": batch_id.split("_")[1] if "_" in batch_id else batch_id,
            "answers": json.dumps(answers),
            "scores": json.dumps({"avg": round(sum(answers.values()) / len(answers), 2)}),
            "duration_seconds": rng.randint(60, 300),
            "created_at": ts.isoformat(),
        })

        user.completed_batches.append(batch_id)
        user.assessments_completed += 1

        # æ›´æ–° session
        for rec in self.records["assessment_sessions"]:
            if rec["user_id"] == user.id:
                rec["completed_batches"] = json.dumps(user.completed_batches)
                rec["total_questions_answered"] = sum(
                    [7,5,13,12,20,5,18,25,25,16,16,14][:len(user.completed_batches)])
                rec["last_activity"] = ts.isoformat()
                if len(user.completed_batches) >= 12:
                    rec["status"] = "completed"
                    rec["completed_at"] = ts.isoformat()
                break

    # â”€â”€ è¯„ä¼°æ¨è¿› â”€â”€

    def _do_assessment_batch(self, user: SimUser, day: int):
        BATCH_ORDER = [
            "B1_TTM7_CORE", "B2_SPI_QUICK", "B3_SPI_TRIGGERS", "B4_SPI_TRIGGERS2",
            "B5_SPI_PSY", "B6_SPI_URGENCY", "B7_BPT6", "B8_BIG5_PART1",
            "B9_BIG5_PART2", "B10_CAPACITY_PART1", "B11_CAPACITY_PART2", "B12_TTM7_DEEP",
        ]
        BATCH_Q_COUNTS = [7, 5, 13, 12, 20, 5, 18, 25, 25, 16, 16, 14]

        for i, bid in enumerate(BATCH_ORDER):
            if bid not in user.completed_batches:
                ts = self.ts_rand(day)
                self._submit_batch(user, bid, BATCH_Q_COUNTS[i], day, ts)
                self._add_points(user, "assessment_complete", "growth", 15,
                               f"å®Œæˆè¯„ä¼° {bid}", day)
                break

    # â”€â”€ æ‰“å¡ â”€â”€

    def _do_checkin(self, user: SimUser, day: int, ts: datetime):
        # è¿ç»­æ‰“å¡
        if user.last_checkin_day == day - 1:
            user.streak_days += 1
        elif user.last_checkin_day != day:
            user.streak_days = 1
        user.last_checkin_day = day
        user.longest_streak = max(user.longest_streak, user.streak_days)

        pts = 5
        if user.streak_days >= 30:
            pts = 15
        elif user.streak_days >= 7:
            pts = 10

        self._add_points(user, "daily_checkin", "growth", pts,
                        f"æ¯æ—¥æ‰“å¡(è¿ç»­{user.streak_days}å¤©)", day)

    # â”€â”€ æ—¥å¸¸å¹²é¢„æ•ˆæœ â”€â”€

    def _do_daily_outcome(self, user: SimUser, day: int, ts: datetime):
        params = user.params
        cr_lo, cr_hi = params["completion_rate"]
        completion = round(rng.uniform(cr_lo, cr_hi), 2)

        tasks_assigned = rng.randint(3, 6)
        tasks_completed = int(tasks_assigned * completion)
        tasks_skipped = tasks_assigned - tasks_completed

        # SPI å¾®è°ƒ
        spi_before = user.spi_score
        delta = (completion - 0.5) * rng.uniform(0.2, 1.0)
        user.spi_score = round(max(5, min(95, user.spi_score + delta)), 1)

        # PDCA
        if completion >= 0.8:
            pdca = "maintain"
        elif completion >= 0.5:
            pdca = "adjust"
        else:
            pdca = "downgrade"

        mood = rng.randint(1, 5) if rng.random() > 0.3 else None
        difficulty = rng.randint(1, 5) if rng.random() > 0.4 else None

        self.records["intervention_outcomes"].append({
            "id": self.next_id("intervention_outcomes"),
            "user_id": user.id,
            "outcome_type": "daily",
            "period_start": ts.replace(hour=0, minute=0, second=0).isoformat(),
            "period_end": ts.replace(hour=23, minute=59, second=59).isoformat(),
            "completion_rate": completion,
            "streak_days": user.streak_days,
            "tasks_assigned": tasks_assigned,
            "tasks_completed": tasks_completed,
            "tasks_skipped": tasks_skipped,
            "spi_before": spi_before,
            "spi_after": user.spi_score,
            "spi_delta": round(user.spi_score - spi_before, 2),
            "stage_before": user.stage_str,
            "stage_after": user.stage_str,
            "readiness_before": READINESS[min(user.readiness - 1, 4)],
            "readiness_after": READINESS[min(user.readiness - 1, 4)],
            "cultivation_stage": CULTIVATION[min(user.cultivation, 3)],
            "user_mood": mood,
            "user_difficulty": difficulty,
            "user_notes": "",
            "adjustment_action": pdca,
            "adjustment_detail": json.dumps({"reason": pdca}),
            "effectiveness_score": round(completion * user.spi_score / 100, 3),
            "created_at": ts.isoformat(),
        })

        user.tasks_completed_total += tasks_completed

        if tasks_completed > 0:
            self._add_points(user, "task_complete", "growth",
                           tasks_completed * 3, f"å®Œæˆ{tasks_completed}é¡¹ä»»åŠ¡", day)

    # â”€â”€ å¯¹è¯ â”€â”€

    def _do_chat(self, user: SimUser, day: int,
                intent: str = None, msg: str = None, resp: str = None):
        ts = self.ts_rand(day)

        if intent is None:
            # æŒ‰ç”»åƒåˆ†é…æ„å›¾
            if user.archetype == "explorer":
                intent_weights = [("knowledge_query", 0.5), ("emotional_support", 0.15),
                                 ("prescription_adjust", 0.15), ("casual", 0.2)]
            elif user.archetype == "struggling":
                intent_weights = [("knowledge_query", 0.2), ("emotional_support", 0.45),
                                 ("prescription_adjust", 0.2), ("casual", 0.15)]
            else:
                intent_weights = [("knowledge_query", 0.35), ("emotional_support", 0.2),
                                 ("prescription_adjust", 0.2), ("casual", 0.25)]
            intent = rng.weighted_choice(intent_weights)

        if msg is None:
            msg = rng.choice(CHAT_TEMPLATES.get(intent, CHAT_TEMPLATES["casual"]))
        if resp is None:
            resp = rng.choice(COACH_RESPONSES.get(intent, COACH_RESPONSES["casual"]))

        # æ¨¡å‹è·¯ç”±
        if intent == "casual":
            model, provider, complexity = "deepseek-chat", "deepseek", "simple"
            in_tok, out_tok = rng.randint(10, 50), rng.randint(20, 80)
            cost = round(in_tok * 0.0001 + out_tok * 0.0002, 4)
            latency = rng.randint(200, 800)
        elif intent == "knowledge_query":
            model, provider, complexity = "qwen-plus", "dashscope", "moderate"
            in_tok, out_tok = rng.randint(100, 400), rng.randint(200, 600)
            cost = round(in_tok * 0.0008 + out_tok * 0.002, 4)
            latency = rng.randint(800, 3000)
        elif intent == "prescription_adjust":
            model, provider, complexity = "qwen-max", "dashscope", "complex"
            in_tok, out_tok = rng.randint(300, 800), rng.randint(400, 1000)
            cost = round(in_tok * 0.002 + out_tok * 0.006, 4)
            latency = rng.randint(1500, 5000)
        else:
            model, provider, complexity = "qwen-plus", "dashscope", "moderate"
            in_tok, out_tok = rng.randint(80, 300), rng.randint(150, 500)
            cost = round(in_tok * 0.0008 + out_tok * 0.002, 4)
            latency = rng.randint(600, 2500)

        fell_back = rng.random() < 0.03  # 3% é™çº§ç‡
        if fell_back:
            model = "deepseek-chat"
            provider = "deepseek"

        session_id = f"sess_{user.id}_{day}_{rng.randint(1000,9999)}"

        # LLM æ—¥å¿—
        self.records["llm_call_logs"].append({
            "id": self.next_id("llm_call_logs"),
            "user_id": user.id, "session_id": session_id,
            "created_at": ts.isoformat(),
            "intent": intent, "complexity": complexity,
            "model_requested": model if not fell_back else "qwen-plus",
            "model_actual": model, "provider": provider,
            "fell_back": fell_back,
            "input_tokens": in_tok, "output_tokens": out_tok,
            "cost_yuan": cost, "latency_ms": latency,
            "finish_reason": "stop",
            "user_message_preview": msg[:100],
            "assistant_message_preview": resp[:100],
            "error_message": None,
        })

        # RAG æ—¥å¿— (çŸ¥è¯†æŸ¥è¯¢ç±»)
        if intent == "knowledge_query":
            results_count = rng.randint(2, 5)
            top_score = round(rng.uniform(0.65, 0.95), 3)
            self.records["rag_query_logs"].append({
                "id": self.next_id("rag_query_logs"),
                "user_id": user.id, "created_at": ts.isoformat(),
                "query_text": msg, "query_type": "knowledge",
                "doc_type_filter": None, "top_k": 5,
                "results_count": results_count,
                "top_score": top_score,
                "avg_score": round(top_score * rng.uniform(0.7, 0.9), 3),
                "sources_json": json.dumps([f"kc_{rng.randint(1,50):04d}" for _ in range(results_count)]),
                "total_latency_ms": rng.randint(50, 300),
                "llm_call_log_id": self.counters["llm_call_logs"],
            })

    # â”€â”€ é˜¶æ®µè¿›é˜¶ â”€â”€

    def _do_stage_advance(self, user: SimUser, day: int):
        if user.current_stage >= 6:
            return
        if user.spi_score < 20 + user.current_stage * 10:
            return  # SPI ä¸å¤Ÿ

        old_stage = user.current_stage
        user.current_stage += 1
        ts = self.ts_rand(day)

        self.records["stage_transition_logs"].append({
            "id": self.next_id("stage_transition_logs"),
            "user_id": user.id, "transition_type": "behavioral",
            "from_value": STAGES[old_stage], "to_value": STAGES[user.current_stage],
            "trigger": "spi_threshold",
            "evidence": json.dumps({
                "spi_score": user.spi_score,
                "days_in_stage": rng.randint(7, 45),
                "completion_rate_avg": round(rng.uniform(0.5, 0.9), 2),
            }),
            "created_at": ts.isoformat(),
        })

        # æˆé•¿ç­‰çº§è”åŠ¨
        new_gl = min(user.current_stage // 2 + (1 if user.growth_points > 300 else 0), 4)
        if new_gl > user.growth_level:
            old_gl = user.growth_level
            user.growth_level = new_gl
            self.records["stage_transition_logs"].append({
                "id": self.next_id("stage_transition_logs"),
                "user_id": user.id, "transition_type": "growth",
                "from_value": GROWTH_LEVELS[old_gl], "to_value": GROWTH_LEVELS[new_gl],
                "trigger": "stage_advance",
                "evidence": json.dumps({"growth_points": user.growth_points}),
                "created_at": ts.isoformat(),
            })

        # å¯è¯»æ€§æå‡
        if user.readiness < 5 and rng.random() < 0.5:
            old_r = user.readiness
            user.readiness += 1
            self.records["stage_transition_logs"].append({
                "id": self.next_id("stage_transition_logs"),
                "user_id": user.id, "transition_type": "readiness",
                "from_value": READINESS[old_r - 1], "to_value": READINESS[user.readiness - 1],
                "trigger": "behavioral_advance",
                "evidence": json.dumps({}),
                "created_at": ts.isoformat(),
            })

        # åŸ¹å…»é˜¶æ®µ
        days_total = day - user.register_day
        new_cult = 0 if days_total < 30 else 1 if days_total < 90 else 2 if days_total < 150 else 3
        if new_cult > user.cultivation:
            old_c = user.cultivation
            user.cultivation = new_cult
            self.records["stage_transition_logs"].append({
                "id": self.next_id("stage_transition_logs"),
                "user_id": user.id, "transition_type": "cultivation",
                "from_value": CULTIVATION[old_c], "to_value": CULTIVATION[new_cult],
                "trigger": "time_elapsed",
                "evidence": json.dumps({"days_total": days_total}),
                "created_at": ts.isoformat(),
            })

        self._add_points(user, "stage_advance", "growth", 50,
                        f"é˜¶æ®µè¿›é˜¶ {STAGES[old_stage]}â†’{STAGES[user.current_stage]}", day)

    # â”€â”€ ç§¯åˆ† â”€â”€

    def _add_points(self, user: SimUser, event_type: str, dimension: str,
                   points: int, desc: str, day: int):
        self.records["point_events"].append({
            "id": self.next_id("point_events"),
            "user_id": user.id, "event_type": event_type,
            "dimension": dimension, "points": points,
            "source_type": event_type.split("_")[0],
            "source_id": f"{event_type}_{user.id}_{day}",
            "description": desc,
            "created_at": self.ts_rand(day).isoformat(),
        })
        if dimension == "growth":
            user.growth_points += points
        elif dimension == "contribution":
            user.contribution_points += points
        elif dimension == "influence":
            user.influence_points += points

    # â”€â”€ ç”Ÿæˆç§¯åˆ†ä½™é¢å¿«ç…§ â”€â”€

    def gen_point_balances(self):
        print("  [ç§¯åˆ†] user_point_balances")
        for user in self.users:
            total = user.growth_points + user.contribution_points + user.influence_points
            self.records["user_point_balances"].append({
                "user_id": user.id,
                "growth": user.growth_points,
                "contribution": user.contribution_points,
                "influence": user.influence_points,
                "total": total,
                "streak_days": user.streak_days,
                "longest_streak": user.longest_streak,
                "last_checkin_date": (self.ts(user.last_checkin_day).isoformat()
                                     if user.last_checkin_day >= 0 else None),
                "tasks_completed_total": user.tasks_completed_total,
                "assessments_completed": user.assessments_completed,
                "updated_at": self.ts(self.cfg.total_days - 1).isoformat(),
            })

    # â”€â”€ å¥–åŠ±å…‘æ¢ â”€â”€

    def gen_user_rewards(self):
        print("  [å¥–åŠ±] user_rewards")
        for user in self.users:
            for reward in REWARDS_CATALOG:
                rtype, name, desc, icon, dim, thresh = reward
                pts = getattr(user, f"{dim}_points", 0) if dim != "streak" else user.longest_streak
                if pts >= thresh:
                    self.records["user_rewards"].append({
                        "id": self.next_id("user_rewards"),
                        "user_id": user.id,
                        "reward_id": REWARDS_CATALOG.index(reward) + 1,
                        "earned_at": self.ts(
                            min(user.register_day + rng.randint(1, 90),
                                self.cfg.total_days - 1)).isoformat(),
                    })

    # â”€â”€ å‘¨æŠ¥ (æ¯ 7 å¤©æ±‡æ€») â”€â”€

    def gen_weekly_reviews(self):
        print("  [å‘¨æŠ¥] weekly intervention_outcomes")
        for user in self.users:
            reg = user.register_day
            for week_start in range(reg + 7, self.cfg.total_days, 7):
                # æ±‡æ€»è¯¥å‘¨æ•°æ®
                week_outcomes = [
                    r for r in self.records["intervention_outcomes"]
                    if r["user_id"] == user.id and
                    self.ts(week_start - 7).isoformat() <= r["period_start"] < self.ts(week_start).isoformat()
                ]
                if not week_outcomes:
                    continue

                avg_cr = sum(o["completion_rate"] for o in week_outcomes) / len(week_outcomes)
                spi_start = week_outcomes[0]["spi_before"]
                spi_end = week_outcomes[-1]["spi_after"]

                self.records["intervention_outcomes"].append({
                    "id": self.next_id("intervention_outcomes"),
                    "user_id": user.id,
                    "outcome_type": "weekly",
                    "period_start": self.ts(week_start - 7).isoformat(),
                    "period_end": self.ts(week_start - 1, 23, 59).isoformat(),
                    "completion_rate": round(avg_cr, 3),
                    "streak_days": user.streak_days,
                    "tasks_assigned": sum(o["tasks_assigned"] for o in week_outcomes),
                    "tasks_completed": sum(o["tasks_completed"] for o in week_outcomes),
                    "tasks_skipped": sum(o["tasks_skipped"] for o in week_outcomes),
                    "spi_before": spi_start,
                    "spi_after": spi_end,
                    "spi_delta": round(spi_end - spi_start, 2),
                    "stage_before": week_outcomes[0]["stage_before"],
                    "stage_after": week_outcomes[-1]["stage_after"],
                    "readiness_before": week_outcomes[0]["readiness_before"],
                    "readiness_after": week_outcomes[-1]["readiness_after"],
                    "cultivation_stage": week_outcomes[-1]["cultivation_stage"],
                    "user_mood": None, "user_difficulty": None, "user_notes": "",
                    "adjustment_action": "review",
                    "adjustment_detail": json.dumps({"type": "weekly_summary"}),
                    "effectiveness_score": round(avg_cr * spi_end / 100, 3),
                    "created_at": self.ts(week_start).isoformat(),
                })

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¸»è¿è¡Œ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def run(self):
        print("=" * 60)
        print("BHP v3 â€” 180å¤©å…¨æµç¨‹æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ")
        print("=" * 60)

        print("\nğŸ“¦ ç”Ÿæˆç§å­æ•°æ®...")
        self.gen_seed_data()

        print("\nğŸ‘¤ ç”Ÿæˆç”¨æˆ·...")
        self.gen_users()

        print(f"\nğŸ”„ æ¨¡æ‹Ÿ {self.cfg.total_days} å¤©è¿è¥...")
        daily_stats = []
        for day in range(self.cfg.total_days):
            dau = self.simulate_day(day)
            daily_stats.append(dau)
            if (day + 1) % 30 == 0 or day == 0:
                print(f"  Day {day+1:3d}: DAU={dau:4d}  "
                      f"æ³¨å†Œ={sum(1 for u in self.users if u.register_day <= day):5d}  "
                      f"æ´»è·ƒ={sum(1 for u in self.users if not u.dropped_out and u.register_day <= day):5d}  "
                      f"æµå¤±={sum(1 for u in self.users if u.dropped_out):4d}")

        print("\nğŸ“Š ç”Ÿæˆæ±‡æ€»æ•°æ®...")
        self.gen_point_balances()
        self.gen_user_rewards()
        self.gen_weekly_reviews()

        # ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        total_records = 0
        for table, rows in sorted(self.records.items()):
            print(f"  {table:40s} {len(rows):>8,} æ¡")
            total_records += len(rows)
        print(f"  {'â”€' * 40} {'â”€' * 8}")
        print(f"  {'æ€»è®¡':40s} {total_records:>8,} æ¡")
        print(f"\n  å¹³å‡ DAU: {sum(daily_stats) / len(daily_stats):.0f}")
        print(f"  æœ€é«˜ DAU: {max(daily_stats)}")
        print(f"  æœ€ä½ DAU: {min(daily_stats)}")

        # ç”»åƒåˆ†å¸ƒ
        arch_counts = defaultdict(int)
        for u in self.users:
            arch_counts[u.archetype] += 1
        print(f"\n  ç”¨æˆ·ç”»åƒåˆ†å¸ƒ:")
        for a, c in sorted(arch_counts.items()):
            print(f"    {a:15s} {c:5d} ({c/len(self.users)*100:.1f}%)")

        # é˜¶æ®µåˆ†å¸ƒ (æœ€ç»ˆ)
        stage_counts = defaultdict(int)
        for u in self.users:
            stage_counts[u.stage_str] += 1
        print(f"\n  æœ€ç»ˆé˜¶æ®µåˆ†å¸ƒ:")
        for s in STAGES:
            c = stage_counts.get(s, 0)
            print(f"    {s}: {c:5d} ({c/len(self.users)*100:.1f}%)")

        return self.records, daily_stats


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®åº“å†™å…¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def write_to_db(records: dict, db_url: str = None):
    """æ‰¹é‡å†™å…¥æ•°æ®åº“ (è‡ªåŒ…å«å»ºè¡¨, ä¸ä¾èµ– ORM metadata)"""
    if db_url is None:
        db_url = os.environ.get("DATABASE_URL", "sqlite:///bhp_sim_180d.db")

    print(f"\nğŸ’¾ å†™å…¥æ•°æ®åº“: {db_url.split('@')[-1] if '@' in db_url else db_url}")

    engine = create_engine(db_url)

    # æ‰€æœ‰å»ºè¡¨ DDL (SQLite å…¼å®¹, PG ä¹ŸåŸºæœ¬å…¼å®¹)
    DDL_STATEMENTS = [
        """CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY, phone VARCHAR(20) UNIQUE,
            username VARCHAR(50) UNIQUE NOT NULL, email VARCHAR(120) NOT NULL,
            password_hash VARCHAR(128) NOT NULL, nickname VARCHAR(64) DEFAULT '',
            avatar_url VARCHAR(256) DEFAULT '', role VARCHAR(32) DEFAULT 'OBSERVER',
            is_active BOOLEAN DEFAULT true, health_competency_level VARCHAR(4) DEFAULT 'Lv0',
            current_stage VARCHAR(4) DEFAULT 'S0', growth_level VARCHAR(4) DEFAULT 'G0',
            created_at TIMESTAMP DEFAULT NOW(), updated_at TIMESTAMP DEFAULT NOW(),
            last_login_at TIMESTAMP)""",
        """CREATE TABLE IF NOT EXISTS change_causes (
            id VARCHAR(4) PRIMARY KEY, category VARCHAR(20), name_zh VARCHAR(50),
            name_en VARCHAR(50), description TEXT, assessment_question TEXT, weight FLOAT DEFAULT 1.0)""",
        """CREATE TABLE IF NOT EXISTS user_change_cause_scores (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            assessment_id INTEGER, cause_code VARCHAR(32), raw_score INTEGER DEFAULT 0,
            weighted_score FLOAT DEFAULT 0)""",
        """CREATE TABLE IF NOT EXISTS intervention_strategies (
            id INTEGER PRIMARY KEY, stage_code VARCHAR(8), readiness_level VARCHAR(8),
            cause_code VARCHAR(32), cause_name VARCHAR(64), strategy_type VARCHAR(64),
            coach_script TEXT, priority INTEGER DEFAULT 0, bpt_tags TEXT, domain VARCHAR(32))""",
        """CREATE TABLE IF NOT EXISTS health_competency_assessments (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            total_score FLOAT, level VARCHAR(4), dimension_scores TEXT, created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS comb_assessments (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            capability FLOAT, opportunity FLOAT, motivation FLOAT,
            bottleneck VARCHAR(32), created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS self_efficacy_assessments (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            total_score FLOAT, level VARCHAR(16), created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS obstacle_assessments (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            total_score FLOAT, top_obstacles TEXT, prescription_adjustments TEXT, created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS support_assessments (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            support_level VARCHAR(16), layer_scores TEXT, weakest_layer VARCHAR(32), created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS intervention_outcomes (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            outcome_type VARCHAR(20), period_start DATETIME, period_end DATETIME,
            completion_rate FLOAT, streak_days INTEGER DEFAULT 0,
            tasks_assigned INTEGER DEFAULT 0, tasks_completed INTEGER DEFAULT 0,
            tasks_skipped INTEGER DEFAULT 0, spi_before FLOAT, spi_after FLOAT,
            spi_delta FLOAT, stage_before VARCHAR(4), stage_after VARCHAR(4),
            readiness_before VARCHAR(4), readiness_after VARCHAR(4),
            cultivation_stage VARCHAR(20), user_mood INTEGER, user_difficulty INTEGER,
            user_notes TEXT DEFAULT '', adjustment_action VARCHAR(16),
            adjustment_detail TEXT, effectiveness_score FLOAT, created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS stage_transition_logs (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            transition_type VARCHAR(20), from_value VARCHAR(10), to_value VARCHAR(10),
            trigger VARCHAR(50), evidence TEXT, created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS point_events (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            event_type VARCHAR(30), dimension VARCHAR(15), points INTEGER,
            source_type VARCHAR(30), source_id VARCHAR(50), description VARCHAR(200),
            created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS user_point_balances (
            user_id INTEGER PRIMARY KEY REFERENCES users(id),
            growth INTEGER DEFAULT 0, contribution INTEGER DEFAULT 0,
            influence INTEGER DEFAULT 0, total INTEGER DEFAULT 0,
            streak_days INTEGER DEFAULT 0, longest_streak INTEGER DEFAULT 0,
            last_checkin_date DATETIME, tasks_completed_total INTEGER DEFAULT 0,
            assessments_completed INTEGER DEFAULT 0, updated_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS incentive_rewards (
            id INTEGER PRIMARY KEY, reward_type VARCHAR(30), name VARCHAR(50),
            description TEXT, icon VARCHAR(10), unlock_dimension VARCHAR(15),
            unlock_threshold INTEGER)""",
        """CREATE TABLE IF NOT EXISTS user_rewards (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            reward_id INTEGER REFERENCES incentive_rewards(id),
            points_spent INTEGER DEFAULT 0, created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS assessment_sessions (
            id INTEGER PRIMARY KEY, user_id INTEGER REFERENCES users(id),
            status VARCHAR(15) DEFAULT 'in_progress', completed_batches TEXT DEFAULT '[]',
            pending_batches TEXT DEFAULT '[]', total_questions_answered INTEGER DEFAULT 0,
            total_questions INTEGER DEFAULT 176, partial_results TEXT DEFAULT '{}',
            started_at DATETIME, last_activity DATETIME, completed_at DATETIME, expires_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS batch_answers (
            id INTEGER PRIMARY KEY, session_id INTEGER REFERENCES assessment_sessions(id),
            user_id INTEGER REFERENCES users(id), batch_id VARCHAR(30),
            questionnaire VARCHAR(10), answers TEXT, scores TEXT,
            duration_seconds INTEGER DEFAULT 0, created_at DATETIME)""",
        """CREATE TABLE IF NOT EXISTS llm_call_logs (
            id INTEGER PRIMARY KEY, user_id INTEGER, session_id VARCHAR(64),
            created_at DATETIME, intent VARCHAR(32), complexity VARCHAR(16),
            model_requested VARCHAR(64), model_actual VARCHAR(64), provider VARCHAR(32),
            fell_back BOOLEAN DEFAULT 0, input_tokens INTEGER DEFAULT 0,
            output_tokens INTEGER DEFAULT 0, cost_yuan FLOAT DEFAULT 0,
            latency_ms INTEGER DEFAULT 0, finish_reason VARCHAR(32),
            user_message_preview TEXT, assistant_message_preview TEXT, error_message TEXT)""",
        """CREATE TABLE IF NOT EXISTS rag_query_logs (
            id INTEGER PRIMARY KEY, user_id INTEGER, created_at DATETIME,
            query_text TEXT, query_type VARCHAR(32), doc_type_filter VARCHAR(32),
            top_k INTEGER DEFAULT 5, results_count INTEGER DEFAULT 0,
            top_score FLOAT DEFAULT 0, avg_score FLOAT DEFAULT 0,
            sources_json TEXT, total_latency_ms INTEGER DEFAULT 0, llm_call_log_id INTEGER)""",
        """CREATE TABLE IF NOT EXISTS knowledge_chunks (
            id INTEGER PRIMARY KEY, chunk_id VARCHAR(128) UNIQUE,
            source VARCHAR(256), doc_type VARCHAR(32), section VARCHAR(256),
            seq INTEGER DEFAULT 0, char_count INTEGER DEFAULT 0,
            text_preview TEXT, created_at DATETIME, updated_at DATETIME)""",
    ]

    # Skip DDL for PostgreSQL (tables already exist with correct schema)
    if "postgresql" not in db_url:
        with engine.begin() as conn:
            for ddl in DDL_STATEMENTS:
                conn.execute(text(ddl))

    # å†™å…¥é¡ºåº (æŒ‰å¤–é”®ä¾èµ–; skip knowledge_chunks â€” different schema in prod)
    TABLE_ORDER = [
        "users", "change_causes", "intervention_strategies",
        "incentive_rewards",
        "assessment_sessions",
        "user_change_cause_scores", "health_competency_assessments",
        "comb_assessments", "self_efficacy_assessments",
        "obstacle_assessments", "support_assessments",
        "batch_answers",
        "intervention_outcomes", "stage_transition_logs",
        "point_events", "user_point_balances",
        "user_rewards",
        "llm_call_logs", "rag_query_logs",
    ]

    # For PostgreSQL: delete sim data before re-inserting
    # Only preserve seed users (id < 1001); all other tables fully cleared
    # Explicit FK-safe delete order (children before parents)
    DELETE_ORDER = [
        "batch_answers",                     # â†’ assessment_sessions, users
        "user_change_cause_scores",          # â†’ users, change_causes
        "health_competency_assessments",     # â†’ users
        "comb_assessments",                  # â†’ users
        "self_efficacy_assessments",         # â†’ users
        "obstacle_assessments",              # â†’ users
        "support_assessments",               # â†’ users
        "user_rewards",                      # â†’ users, incentive_rewards
        "point_events",                      # â†’ users
        "user_point_balances",               # â†’ users
        "intervention_outcomes",             # â†’ users
        "stage_transition_logs",             # â†’ users
        "llm_call_logs",                     # no FK
        "rag_query_logs",                    # no FK
        "assessment_sessions",               # â†’ users
        "incentive_rewards",                 # parent
        "intervention_strategies",           # parent
        "change_causes",                     # parent
        "users",                             # root
    ]
    SIM_ID_OFFSET = 1001
    print("  ğŸ—‘ï¸ æ¸…ç†æ—§æ•°æ®...")
    with engine.begin() as conn:
        for table_name in DELETE_ORDER:
            rows = records.get(table_name, [])
            if not rows:
                continue
            try:
                if table_name == "users":
                    conn.execute(text(f"DELETE FROM {table_name} WHERE id >= {SIM_ID_OFFSET}"))
                else:
                    conn.execute(text(f"DELETE FROM {table_name}"))
                print(f"    DELETE {table_name} âœ“")
            except Exception as e:
                print(f"    âš ï¸ DELETE {table_name} failed: {e}")

    with engine.begin() as conn:
        for table_name in TABLE_ORDER:
            rows = records.get(table_name, [])
            if not rows:
                continue

            # æ‰¹é‡æ’å…¥ (æ¯æ¬¡ 500 è¡Œ)
            cols = list(rows[0].keys())
            placeholders = ", ".join([f":{c}" for c in cols])
            col_str = ", ".join(cols)
            sql = text(f"INSERT INTO {table_name} ({col_str}) VALUES ({placeholders})")

            batch_size = 500
            for i in range(0, len(rows), batch_size):
                batch = rows[i:i+batch_size]
                conn.execute(sql, batch)

            print(f"  âœ… {table_name}: {len(rows):,} æ¡")

    # Reset users sequence to max(id)+1 so future INSERTs don't conflict
    if "postgresql" in db_url:
        with engine.begin() as conn:
            conn.execute(text("SELECT setval('users_id_seq', (SELECT MAX(id) FROM users))"))

    print("\nâœ… æ•°æ®å†™å…¥å®Œæˆ!")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI å…¥å£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="BHP v3 æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨")
    parser.add_argument("--users", type=int, default=1200, help="æ€»ç”¨æˆ·æ•°")
    parser.add_argument("--days", type=int, default=180, help="æ¨¡æ‹Ÿå¤©æ•°")
    parser.add_argument("--dau", type=int, default=300, help="ç›®æ ‡ DAU")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--no-db", action="store_true", help="åªç”Ÿæˆä¸å†™å…¥")
    parser.add_argument("--json", action="store_true", help="å¯¼å‡º JSON")
    args = parser.parse_args()

    CFG.total_users = args.users
    CFG.total_days = args.days
    CFG.target_dau = args.dau
    CFG.seed = args.seed

    gen = SimDataGenerator(CFG)
    records, stats = gen.run()

    if args.json:
        out_file = "bhp_sim_data.json"
        with open(out_file, "w", encoding="utf-8") as f:
            json.dump({k: v for k, v in records.items()}, f,
                     ensure_ascii=False, indent=2, default=str)
        print(f"\nğŸ“„ JSON å¯¼å‡º: {out_file}")

    if not args.no_db:
        write_to_db(records)
