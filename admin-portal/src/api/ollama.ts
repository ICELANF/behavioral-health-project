/**
 * Ollama API å®¢æˆ·ç«¯ - æœ¬åœ°å¤§æ¨¡å‹å¯¹æ¥
 * ä½¿ç”¨ DeepSeek-R1 æˆ– Qwen2.5 è¿›è¡Œå¥åº·å’¨è¯¢
 */

// Ollama é…ç½®
const OLLAMA_CONFIG = {
  baseUrl: import.meta.env.VITE_OLLAMA_URL || 'http://localhost:11434',
  model: import.meta.env.VITE_OLLAMA_MODEL || 'deepseek-r1:7b',
  // å¤‡ç”¨æ¨¡å‹
  fallbackModel: 'qwen2.5:14b',
}

// ============ ç±»å‹å®šä¹‰ ============

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export interface GenerateResponse {
  model: string
  created_at: string
  response: string
  done: boolean
  context?: number[]
  total_duration?: number
  load_duration?: number
  prompt_eval_count?: number
  prompt_eval_duration?: number
  eval_count?: number
  eval_duration?: number
}

export interface ChatResponse {
  model: string
  created_at: string
  message: ChatMessage
  done: boolean
  total_duration?: number
  eval_count?: number
}

export interface ModelInfo {
  name: string
  modified_at: string
  size: number
  digest: string
  details: {
    format: string
    family: string
    parameter_size: string
    quantization_level: string
  }
}

// ============ ç³»ç»Ÿæç¤ºè¯ ============

const HEALTH_COACH_SYSTEM_PROMPT = `# è§’è‰²å®šä¹‰
ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¡Œä¸ºå¥åº·æ•™ç»ƒï¼Œä¸“æ³¨äºç³–å°¿ç—…ç®¡ç†å’Œæ…¢æ€§ç—…è¡Œä¸ºå¹²é¢„ã€‚

## TTM è·¨ç†è®ºæ¨¡å‹ - è¡Œä¸ºæ”¹å˜é˜¶æ®µ
1. **å‰æ„å‘æœŸ** - æ— æ”¹å˜æ„æ„¿ï¼Œéœ€è¦æå‡è®¤çŸ¥
2. **æ„å‘æœŸ** - å¼€å§‹è€ƒè™‘æ”¹å˜ï¼Œå¸®åŠ©è¯„ä¼°åˆ©å¼Š
3. **å‡†å¤‡æœŸ** - å‡†å¤‡è¡ŒåŠ¨ï¼Œå¸®åŠ©åˆ¶å®šè®¡åˆ’
4. **è¡ŒåŠ¨æœŸ** - æ­£åœ¨æ”¹å˜ï¼Œæä¾›æ”¯æŒé¼“åŠ±
5. **ç»´æŒæœŸ** - å·²å½¢æˆä¹ æƒ¯ï¼Œé¢„é˜²å¤å‘

## å…³æ³¨é¢†åŸŸ
- ğŸ©¸ è¡€ç³–ç®¡ç†ï¼šç›‘æµ‹ã€è§£è¯»ã€æ³¢åŠ¨åˆ†æ
- ğŸ¥— é¥®é£Ÿæ§åˆ¶ï¼šä½GIé¥®é£Ÿã€é£Ÿç‰©é€‰æ‹©
- ğŸƒ è¿åŠ¨é”»ç‚¼ï¼šæœ‰æ°§è¿åŠ¨ã€è¿åŠ¨å¤„æ–¹
- ğŸ’Š ç”¨è¯ä¾ä»ï¼šæœè¯æé†’ã€è¯ç‰©çŸ¥è¯†
- ğŸ˜´ ç¡çœ è´¨é‡ï¼šä½œæ¯è°ƒæ•´
- ğŸ§˜ å‹åŠ›ç®¡ç†ï¼šæ”¾æ¾æŠ€å·§
- âš–ï¸ ä½“é‡ç®¡ç†ï¼šå‡é‡ç­–ç•¥

## åŠ¨æœºè®¿è°ˆåŸåˆ™
- ä½¿ç”¨å¼€æ”¾å¼é—®é¢˜å¼•å¯¼å¯¹è¯
- è®¤å¯ç”¨æˆ·çš„åŠªåŠ›å’Œè¿›æ­¥
- åæ˜ å¼å€¾å¬ï¼Œç†è§£æ„Ÿå—
- ä¸æ‰¹åˆ¤ä¸è¯´æ•™ï¼Œå°Šé‡è‡ªä¸»æ€§

## å®‰å…¨æç¤º
- è¡€ç³–>16.7æˆ–<3.9 mmol/Lï¼Œå»ºè®®ç«‹å³å°±åŒ»
- ä¸æä¾›å…·ä½“è¯ç‰©è°ƒæ•´å»ºè®®
- è¯†åˆ«è´Ÿé¢æƒ…ç»ªï¼Œå¿…è¦æ—¶å»ºè®®ä¸“ä¸šæ”¯æŒ

## è¾“å‡ºæ ¼å¼
- å›å¤ç®€æ´æ¸©æš–ï¼Œä¸è¶…è¿‡200å­—
- ä½¿ç”¨emojiå¢åŠ äº²å’ŒåŠ›
- é€‚æ—¶æå‡ºå¼•å¯¼æ€§é—®é¢˜`

// ============ API å‡½æ•° ============

/**
 * æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦å¯ç”¨
 */
export async function checkOllamaHealth(): Promise<boolean> {
  try {
    const response = await fetch(`${OLLAMA_CONFIG.baseUrl}/api/tags`)
    return response.ok
  } catch {
    return false
  }
}

/**
 * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
 */
export async function getAvailableModels(): Promise<ModelInfo[]> {
  const response = await fetch(`${OLLAMA_CONFIG.baseUrl}/api/tags`)
  if (!response.ok) {
    throw new Error('è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥')
  }
  const data = await response.json()
  return data.models || []
}

/**
 * æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å¯ç”¨
 */
export async function isModelAvailable(modelName: string): Promise<boolean> {
  try {
    const models = await getAvailableModels()
    return models.some(m => m.name === modelName || m.name.startsWith(modelName.split(':')[0]))
  } catch {
    return false
  }
}

/**
 * ç®€å•ç”Ÿæˆ (å•è½®å¯¹è¯)
 */
export async function generate(
  prompt: string,
  options?: {
    model?: string
    system?: string
    stream?: boolean
    context?: number[]
  }
): Promise<GenerateResponse> {
  const model = options?.model || OLLAMA_CONFIG.model

  const response = await fetch(`${OLLAMA_CONFIG.baseUrl}/api/generate`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model,
      prompt,
      system: options?.system || HEALTH_COACH_SYSTEM_PROMPT,
      stream: options?.stream ?? false,
      context: options?.context,
      options: {
        temperature: 0.7,
        top_p: 0.9,
        num_predict: 512,
      },
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    throw new Error(`ç”Ÿæˆå¤±è´¥: ${error}`)
  }

  return response.json()
}

/**
 * æµå¼ç”Ÿæˆ
 */
export async function* generateStream(
  prompt: string,
  options?: {
    model?: string
    system?: string
    context?: number[]
  }
): AsyncGenerator<string> {
  const model = options?.model || OLLAMA_CONFIG.model

  const response = await fetch(`${OLLAMA_CONFIG.baseUrl}/api/generate`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model,
      prompt,
      system: options?.system || HEALTH_COACH_SYSTEM_PROMPT,
      stream: true,
      context: options?.context,
      options: {
        temperature: 0.7,
        top_p: 0.9,
        num_predict: 512,
      },
    }),
  })

  if (!response.ok) {
    throw new Error('ç”Ÿæˆå¤±è´¥')
  }

  const reader = response.body?.getReader()
  if (!reader) throw new Error('æ— æ³•è¯»å–å“åº”æµ')

  const decoder = new TextDecoder()

  while (true) {
    const { done, value } = await reader.read()
    if (done) break

    const text = decoder.decode(value, { stream: true })
    const lines = text.split('\n').filter(line => line.trim())

    for (const line of lines) {
      try {
        const data: GenerateResponse = JSON.parse(line)
        if (data.response) {
          yield data.response
        }
      } catch {
        // å¿½ç•¥è§£æé”™è¯¯
      }
    }
  }
}

/**
 * å¤šè½®å¯¹è¯ (Chat API)
 */
export async function chat(
  messages: ChatMessage[],
  options?: {
    model?: string
    stream?: boolean
  }
): Promise<ChatResponse> {
  const model = options?.model || OLLAMA_CONFIG.model

  // æ·»åŠ ç³»ç»Ÿæç¤ºè¯
  const messagesWithSystem: ChatMessage[] = [
    { role: 'system', content: HEALTH_COACH_SYSTEM_PROMPT },
    ...messages,
  ]

  const response = await fetch(`${OLLAMA_CONFIG.baseUrl}/api/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model,
      messages: messagesWithSystem,
      stream: options?.stream ?? false,
      options: {
        temperature: 0.7,
        top_p: 0.9,
        num_predict: 512,
      },
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    throw new Error(`å¯¹è¯å¤±è´¥: ${error}`)
  }

  return response.json()
}

/**
 * æµå¼å¤šè½®å¯¹è¯
 */
export async function* chatStream(
  messages: ChatMessage[],
  options?: {
    model?: string
  }
): AsyncGenerator<string> {
  const model = options?.model || OLLAMA_CONFIG.model

  const messagesWithSystem: ChatMessage[] = [
    { role: 'system', content: HEALTH_COACH_SYSTEM_PROMPT },
    ...messages,
  ]

  const response = await fetch(`${OLLAMA_CONFIG.baseUrl}/api/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model,
      messages: messagesWithSystem,
      stream: true,
      options: {
        temperature: 0.7,
        top_p: 0.9,
        num_predict: 512,
      },
    }),
  })

  if (!response.ok) {
    throw new Error('å¯¹è¯å¤±è´¥')
  }

  const reader = response.body?.getReader()
  if (!reader) throw new Error('æ— æ³•è¯»å–å“åº”æµ')

  const decoder = new TextDecoder()

  while (true) {
    const { done, value } = await reader.read()
    if (done) break

    const text = decoder.decode(value, { stream: true })
    const lines = text.split('\n').filter(line => line.trim())

    for (const line of lines) {
      try {
        const data: ChatResponse = JSON.parse(line)
        if (data.message?.content) {
          yield data.message.content
        }
      } catch {
        // å¿½ç•¥è§£æé”™è¯¯
      }
    }
  }
}

// ============ å¥åº·å’¨è¯¢ä¸“ç”¨å‡½æ•° ============

/**
 * å¥åº·å’¨è¯¢å¯¹è¯
 */
export async function healthConsult(
  userMessage: string,
  history: ChatMessage[] = [],
  context?: {
    stage?: string
    focusAreas?: string[]
    recentGlucose?: number
  }
): Promise<{ response: string; messages: ChatMessage[] }> {
  // æ„å»ºä¸Šä¸‹æ–‡å¢å¼ºçš„æ¶ˆæ¯
  let enhancedMessage = userMessage

  if (context) {
    const contextParts: string[] = []
    if (context.stage) {
      contextParts.push(`[ç”¨æˆ·å½“å‰å¤„äº${context.stage}é˜¶æ®µ]`)
    }
    if (context.focusAreas?.length) {
      contextParts.push(`[å…³æ³¨é¢†åŸŸ: ${context.focusAreas.join(', ')}]`)
    }
    if (context.recentGlucose) {
      contextParts.push(`[æœ€è¿‘è¡€ç³–: ${context.recentGlucose} mmol/L]`)
    }
    if (contextParts.length > 0) {
      enhancedMessage = `${contextParts.join(' ')}\n\nç”¨æˆ·è¯´: ${userMessage}`
    }
  }

  const messages: ChatMessage[] = [
    ...history,
    { role: 'user', content: enhancedMessage },
  ]

  const response = await chat(messages)

  return {
    response: response.message.content,
    messages: [
      ...history,
      { role: 'user', content: userMessage },
      { role: 'assistant', content: response.message.content },
    ],
  }
}

/**
 * å¥åº·å’¨è¯¢å¯¹è¯ (æµå¼)
 */
export async function* healthConsultStream(
  userMessage: string,
  history: ChatMessage[] = [],
  context?: {
    stage?: string
    focusAreas?: string[]
    recentGlucose?: number
  }
): AsyncGenerator<string> {
  let enhancedMessage = userMessage

  if (context) {
    const contextParts: string[] = []
    if (context.stage) {
      contextParts.push(`[ç”¨æˆ·å½“å‰å¤„äº${context.stage}é˜¶æ®µ]`)
    }
    if (context.focusAreas?.length) {
      contextParts.push(`[å…³æ³¨é¢†åŸŸ: ${context.focusAreas.join(', ')}]`)
    }
    if (context.recentGlucose) {
      contextParts.push(`[æœ€è¿‘è¡€ç³–: ${context.recentGlucose} mmol/L]`)
    }
    if (contextParts.length > 0) {
      enhancedMessage = `${contextParts.join(' ')}\n\nç”¨æˆ·è¯´: ${userMessage}`
    }
  }

  const messages: ChatMessage[] = [
    ...history,
    { role: 'user', content: enhancedMessage },
  ]

  yield* chatStream(messages)
}
